{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "600e154c",
        "execution_start": 1743981598151,
        "execution_millis": 4020,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "051f8377b8c247a4941c80e87efdd609",
        "deepnote_cell_type": "code"
      },
      "source": "from Moduled_functions import get_data\nfrom Moduled_functions import tranformation\nfrom Moduled_functions import calculate_ema_slope\nfrom Moduled_functions import place_order\nfrom m_email import send_email\nimport pandas as pd\nfrom keras.models import load_model\nfrom sklearn.preprocessing import StandardScaler\nimport schedule\nimport time\nfrom datetime import datetime\nimport os\naccess_token = os.environ[\"ACCESS_TOKEN\"]\n\nbearish_model = load_model('/work/Trained_Models/FX_Bearish_model_2025-02-19.keras')\nbullish_model = load_model('/work/Trained_Models/FX_Bullish_model_2025-02-21.keras')",
      "block_group": "e011e7f7e67449c1b21244e5d1d0acb8",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-06 23:19:59.121956: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-04-06 23:19:59.125396: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-04-06 23:19:59.158354: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-06 23:19:59.158743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-06 23:19:59.159602: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-04-06 23:19:59.164809: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-04-06 23:19:59.165326: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-04-06 23:20:00.048849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "c4a0e5a6",
        "is_code_hidden": true,
        "execution_start": 1743981602235,
        "execution_millis": 344,
        "is_output_hidden": true,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_app_is_code_hidden": true,
        "deepnote_app_is_output_hidden": true,
        "cell_id": "00f21d7de5bc487fb721a1659115a7bc",
        "deepnote_cell_type": "code"
      },
      "source": "import os\n\n# Create directory structure\ndef create_directory_structure():\n    directories = [\n        'data',\n        'data/raw',\n        'data/processed',\n        'data/signals',\n        'data/orders',\n        'data/predictions',\n        'logs',\n        'config'\n    ]\n    \n    for directory in directories:\n        os.makedirs(directory, exist_ok=True)\n        print(f\"Created directory: {directory}\")\n\ncreate_directory_structure()",
      "block_group": "00f21d7de5bc487fb721a1659115a7bc",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": "Created directory: data\nCreated directory: data/raw\nCreated directory: data/processed\nCreated directory: data/signals\nCreated directory: data/orders\nCreated directory: data/predictions\nCreated directory: logs\nCreated directory: config\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "f2248899",
        "is_code_hidden": false,
        "execution_start": 1743981608138,
        "execution_millis": 174,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_app_is_code_hidden": true,
        "cell_id": "adb1ed85b389463e88051db3f863e8a1",
        "deepnote_cell_type": "code"
      },
      "source": "# Combined trading system framework\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport requests\nfrom abc import ABC, abstractmethod\nimport logging\nimport json\nfrom Moduled_functions import get_data\n\n# Configure logging\nlogging.basicConfig(\n    filename='logs/trading_system.log',\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger('TradingSystem')\nclass BaseStrategy(ABC):\n    def __init__(self, name):\n        self.name = name\n        self.signals_path = f'data/signals/signals_{name}.csv'\n        self.trades_path = f'data/orders/trades_{name}.csv'\n        self.config_path = f'config/strategy_{name}.json'\n        self.load_config()\n    \n    def load_config(self):\n        \"\"\"Load strategy configuration\"\"\"\n        try:\n            with open(self.config_path, 'r') as f:\n                self.config = json.load(f)\n        except FileNotFoundError:\n            self.config = self.get_default_config()\n            self.save_config()\n    \n    def save_config(self):\n        \"\"\"Save strategy configuration\"\"\"\n        with open(self.config_path, 'w') as f:\n            json.dump(self.config, f, indent=4)\n    \n    @abstractmethod\n    def get_default_config(self):\n        \"\"\"Get default strategy configuration\"\"\"\n        pass\n    \n    @abstractmethod\n    def generate_signals(self, data):\n        \"\"\"Generate trading signals based on strategy logic\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_order_parameters(self, current_price, signal):\n        \"\"\"Get order parameters based on strategy signal\"\"\"\n        pass\n    \n    def save_signals(self, signals_data):\n        \"\"\"Save strategy signals to history\"\"\"\n        try:\n            if os.path.exists(self.signals_path):\n                existing_signals = pd.read_csv(self.signals_path)\n                signals_data = signals_data.iloc[-1:]\n                updated_signals = pd.concat([existing_signals, signals_data])\n            else:\n                updated_signals = signals_data\n            updated_signals.to_csv(self.signals_path, index=False)\n            logger.info(f\"Signals saved for strategy {self.name}\")\n        except Exception as e:\n            logger.error(f\"Error saving signals for strategy {self.name}: {str(e)}\")\n    \n    def save_trade(self, trade_data):\n        \"\"\"Save trade details to history\"\"\"\n        try:\n            if os.path.exists(self.trades_path):\n                existing_trades = pd.read_csv(self.trades_path)\n                updated_trades = pd.concat([existing_trades, trade_data])\n            else:\n                updated_trades = trade_data\n            updated_trades.to_csv(self.trades_path, index=False)\n            logger.info(f\"Trade saved for strategy {self.name}\")\n        except Exception as e:\n            logger.error(f\"Error saving trade for strategy {self.name}: {str(e)}\")\n\nclass StrategyRegistry:\n    _strategies = {}\n    \n    @classmethod\n    def register(cls, strategy_instance):\n        \"\"\"Register a new strategy\"\"\"\n        cls._strategies[strategy_instance.name] = strategy_instance\n        logger.info(f\"Strategy registered: {strategy_instance.name}\")\n    \n    @classmethod\n    def get_strategy(cls, name):\n        \"\"\"Get a registered strategy by name\"\"\"\n        return cls._strategies.get(name)\n    \n    @classmethod\n    def get_all_strategies(cls):\n        \"\"\"Get all registered strategies\"\"\"\n        return cls._strategies",
      "block_group": "d004a156fdd64a0a98947cf34419eae7",
      "execution_count": 4,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "ade30329",
        "is_code_hidden": false,
        "execution_start": 1743981614410,
        "execution_millis": 1,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_app_is_code_hidden": true,
        "cell_id": "504fb6ab52404f93a5301b732be9f788",
        "deepnote_cell_type": "code"
      },
      "source": "class DataHandler:\n    def __init__(self):\n        self.scaler = None\n        self.raw_data_path = 'data/raw/base_data.csv'\n        self.processed_data_path = 'data/processed/transformed_data.csv'\n    \n    def get_and_prepare_data(self, lookback, access_token):\n        from Moduled_functions import tranformation\n        \"\"\"Get and prepare the base data\"\"\"\n        try:\n            base_data, current_price = get_data(lookback, access_token)\n            #base_data_append=base_data.iloc[-1:]\n\n        # Read existing history\n            try:\n                existing_data = pd.read_csv(self.raw_data_path)\n                existing_data['datetime'] = pd.to_datetime(existing_data['datetime'])\n                base_data['datetime'] = pd.to_datetime(base_data['datetime'])\n                \n                \n                # Add new records\n                new_records = base_data[~base_data['datetime'].isin(existing_data['datetime'])]\n                if not new_records.empty:\n                    updated_data = pd.concat([existing_data, new_records])\n                    updated_data = updated_data.sort_values('datetime')\n                else:\n                    updated_data = existing_data\n            except FileNotFoundError:\n                updated_data = base_data\n\n            updated_data.to_csv(self.raw_data_path, index=False)\n            logger.info(f\"Raw data saved to {self.raw_data_path}\")\n            \n            base_data = tranformation(base_data, 0.0015)            \n            return base_data, current_price\n        except Exception as e:\n            logger.error(f\"Error in get_and_prepare_data: {str(e)}\")\n            raise\n    \n    def calculate_technical_indicators(self, data):\n        \"\"\"Calculate technical indicators\"\"\"\n        try:\n            data['EMA_slope'] = calculate_ema_slope(data, 'Open', 9)\n            data['EMA_slope_15'] = calculate_ema_slope(data, 'Open', 15)\n            data['EMA_slope_60'] = calculate_ema_slope(data, 'Open', 60)\n            data['deviation_'] = data['bullish_move_flag_20'] + data['bearish_move_flag_20']\n            \n            try:\n                # Read existing history\n                existing_data = pd.read_csv(self.processed_data_path)\n                existing_data['datetime'] = pd.to_datetime(existing_data['datetime'])\n                data['datetime'] = pd.to_datetime(data['datetime'])\n                # Add new records\n                new_records = data[~data['datetime'].isin(existing_data['datetime'])]\n                if not new_records.empty:\n                    data = pd.concat([existing_data, new_records])\n                    data = data.sort_values('datetime')\n                else:\n                    data = existing_data\n            except FileNotFoundError:\n                data.to_csv(self.processed_data_path, index=False)\n\n            data.to_csv(self.processed_data_path, index=False)\n\n            logger.info(f\"Processed data saved to {self.processed_data_path}\")\n            return data\n        except Exception as e:\n            logger.error(f\"Error in calculate_technical_indicators: {str(e)}\")\n            raise\n    \n    def normalize_features(self, data):\n        \"\"\"Normalize the feature data\"\"\"\n        try:\n            X = data.iloc[:,9:69]\n            y = data['deviation_']\n            \n            if self.scaler is None:\n                self.scaler = StandardScaler()\n                self.scaler.fit(X)\n            \n            X_normalized = pd.DataFrame(self.scaler.transform(X)).round(1)\n            return X_normalized, y\n        except Exception as e:\n            logger.error(f\"Error in normalize_features: {str(e)}\")\n            raise\nclass OrderManager:\n    def __init__(self, access_token):\n        self.access_token = access_token\n        #self.account_id = account_id\n        self.orders_path = 'data/orders/all_orders.csv'\n    \n    def place_order(self, current_price, stop_loss, take_profit, quantity,account_id):\n        \"\"\"Place an order\"\"\"\n        try:\n            order = place_order(\n                str(current_price),\n                str(round(stop_loss, 5)),\n                str(round(take_profit, 5)),\n                quantity,\n                self.access_token,\n                account_id\n            )\n            logger.info(f\"Order placed: {order['orderCreateTransaction'].get('id')}\")\n            return order\n        except Exception as e:\n            logger.error(f\"Error placing order: {str(e)}\")\n            raise\n    \n    def save_order(self, order_data):\n        \"\"\"Save order details to history\"\"\"\n        try:\n            if os.path.exists(self.orders_path):\n                existing_orders = pd.read_csv(self.orders_path)\n                updated_orders = pd.concat([existing_orders, order_data])\n            else:\n                updated_orders = order_data\n            updated_orders.to_csv(self.orders_path, index=False)\n            logger.info(\"Order saved to history\")\n        except Exception as e:\n            logger.error(f\"Error saving order: {str(e)}\")",
      "block_group": "98e7de400ce44087bb93324ee96e7f8e",
      "execution_count": 8,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "ea36fb62",
        "is_code_hidden": false,
        "execution_start": 1743983419435,
        "execution_millis": 0,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_app_is_code_hidden": true,
        "cell_id": "eb4c0d0fba4740ee82a9e6bcc4475405",
        "deepnote_cell_type": "code"
      },
      "source": "# Let's modify the TradingSystem class to properly merge the prediction data with the base data\nclass TradingSystem:\n    def __init__(self, access_token, bearish_model, bullish_model):\n        self.data_handler = DataHandler()\n        self.order_manager = OrderManager(access_token)\n        self.predictions_data_path = 'data/predictions/predictions_data.csv'\n        \n    def execute(self, lookback=100):\n        try:\n            # Get and prepare data\n            base_data, current_price = self.data_handler.get_and_prepare_data(lookback, self.order_manager.access_token)\n            base_data = self.data_handler.calculate_technical_indicators(base_data)\n            X_normalized, y = self.data_handler.normalize_features(base_data)\n            \n            # Generate predictions\n            y_pred_bearish = bearish_model.predict(X_normalized, verbose=0)\n            y_pred_bullish = bullish_model.predict(X_normalized, verbose=0)\n\n            # Ensure datetime is unique before combining results\n            base_data = base_data.drop_duplicates(subset='datetime').reset_index(drop=True)\n            \n            # Combine results with datetime from base_data\n            result_combined = pd.concat([\n                base_data['datetime'].reset_index(drop=True),\n                pd.DataFrame(y_pred_bearish, columns=['bearish_0', 'bearish_1', 'bearish_2', 'bearish_3']),\n                pd.DataFrame(y_pred_bullish, columns=['bullish_0', 'bullish_1', 'bullish_2', 'bullish_3']),\n                pd.Series(y, name='y_true')\n            ], axis=1)\n            \n            try:\n                # Read existing predictions history\n                existing_preds = pd.read_csv(self.predictions_data_path)\n                existing_preds['datetime'] = pd.to_datetime(existing_preds['datetime'])\n                result_combined['datetime'] = pd.to_datetime(result_combined['datetime'])\n                \n                # Add new predictions\n                new_preds = result_combined[~result_combined['datetime'].isin(existing_preds['datetime'])]\n                if not new_preds.empty:\n                    updated_preds = pd.concat([existing_preds, new_preds])\n                    updated_preds = updated_preds.sort_values('datetime').reset_index(drop=True)\n                else:\n                    updated_preds = existing_preds\n            except FileNotFoundError:\n                updated_preds = result_combined\n            \n            updated_preds.to_csv(self.predictions_data_path, index=False)\n            \n            # Merge predictions with base data\n            base_data['datetime'] = pd.to_datetime(base_data['datetime'])\n            data_with_predictions = pd.merge(\n                base_data.drop_duplicates(subset='datetime'),\n                updated_preds,\n                on='datetime',\n                how='inner'\n            )\n            \n            # Process each strategy\n            for strategy in StrategyRegistry.get_all_strategies().values():\n                self._process_strategy(strategy, data_with_predictions, current_price)\n            \n            logger.info(f\"Execution completed at {datetime.now()}\")\n            \n        except Exception as e:\n            logger.error(f\"Error in execute: {str(e)}\")\n            raise\n    \n    def _process_strategy(self, strategy, data, current_price):\n        try:\n            # Generate signals\n            signals = strategy.generate_signals(data)\n            signals['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            strategy.save_signals(signals)\n            \n            # Check signals and execute trades\n            if not signals.empty:\n                latest_bearish = signals['bearish_flag'].iloc[-1]\n                latest_bullish = signals['bullish_flag'].iloc[-1]\n                \n                if latest_bullish == 1 and latest_bearish != 2:\n                    self._execute_trade(strategy, 1, current_price)\n                elif latest_bearish == 2 and latest_bullish != 1:\n                    self._execute_trade(strategy, 2, current_price)\n                    \n        except Exception as e:\n            logger.error(f\"Error processing strategy {strategy.name}: {str(e)}\")\n    \n    def _execute_trade(self, strategy, signal, current_price):\n        try:\n            order_params = strategy.get_order_parameters(current_price, signal)\n            if not order_params:\n                return\n            \n            order = self.order_manager.place_order(\n                current_price,\n                order_params['stop_loss'],\n                order_params['take_profit'],\n                order_params['quantity'],\n                order_params['account_id']\n            )\n            \n            order_id = order['orderCreateTransaction'].get('id')\n            order_time = order['orderCreateTransaction'].get('time')\n            account_id = order['orderCreateTransaction'].get('accountID')\n            \n            # Save trade details\n            trade_data = pd.DataFrame({\n                'Time': [order_time],\n                'Strategy': [strategy.name],\n                'Type': [order_params['type']],\n                'Quantity': [order_params['quantity']],\n                'Price': [current_price],\n                'Take_Profit': [order_params['take_profit']],\n                'Stop_Loss': [order_params['stop_loss']],\n                'Order_ID': [order_id],\n                'Account_ID': [account_id],\n            })\n            \n            strategy.save_trade(trade_data)\n            self.order_manager.save_order(trade_data)\n            \n            # Send notification\n            message = f\"{strategy.name} {order_params['type']} Order placed with order no: {order_id}\"\n            requests.get(f'https://api.day.app/iFbt9PqBdm6d2YvUT4irnN/{order_params[\"type\"]} Order Placed/{message}')\n            logger.info(message)\n            \n        except Exception as e:\n            logger.error(f\"Error executing trade for strategy {strategy.name}: {str(e)}\")\n\nprint(\"Trading system framework initialized with directory structure\")",
      "block_group": "dc265a2ae0a043018ce77028bce37b98",
      "execution_count": 42,
      "outputs": [
        {
          "name": "stdout",
          "text": "Trading system framework initialized with directory structure\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "aa20c060",
        "is_code_hidden": false,
        "execution_start": 1743981628465,
        "execution_millis": 0,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_app_is_code_hidden": true,
        "cell_id": "8e1f6e96a5094d3992fb9e5d9fa5839b",
        "deepnote_cell_type": "code"
      },
      "source": "# Add the missing get_order_parameters method to all strategy classes\nclass VolumeBasedStrategy(BaseStrategy):\n    def __init__(self):\n        super().__init__('volume_based')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': 450,\n            'ema_slope_15_threshold_max': 750,\n            'ema_slope_15_threshold_min': -750,\n            'ema_slope_60_threshold': 0.50,\n            'bearish_threshold_min': 0.5,\n            'bearish_threshold_max': 0.65,\n            'volume_threshold': 500,\n            'take_profit_multiplier': 1.0015,\n            'stop_loss_multiplier': 0.0005,\n            'quantity': '1000000',\n            'account_id': account_id\n        }\n    \n    def generate_signals(self, data):\n        config = self.config\n        \n        bearish_signals = data.apply(lambda x: 2 if (\n            x['bearish_2'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        bullish_signals = data.apply(lambda x: 1 if (\n            x['bullish_1'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n        \n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n    \n    def get_order_parameters(self, current_price, signal):\n        config = self.config\n        \n        if signal == 1:  # Bullish\n            return {\n                'take_profit': current_price * config['take_profit_multiplier'],\n                'stop_loss': config['stop_loss_multiplier'],\n                'quantity': config['quantity'],\n                'type': 'BUY',\n                'account_id': config['account_id']\n            }\n        elif signal == 2:  # Bearish\n            return {\n                'take_profit': current_price * (1 - config['stop_loss_multiplier']),\n                'stop_loss': config['stop_loss_multiplier'],\n                'quantity': f\"-{config['quantity']}\",\n                'type': 'SELL',\n                'account_id': config['account_id']\n            }\n        return None\n\nclass VolumeBasedStrategyHedge(BaseStrategy):\n    def __init__(self):\n        super().__init__('volume_based_hedge')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': 450,\n            'ema_slope_15_threshold_max': 750,\n            'ema_slope_15_threshold_min': -750,\n            'ema_slope_60_threshold': 0.50,\n            'bearish_threshold_min': 0.5,\n            'bearish_threshold_max': 0.65,\n            'volume_threshold': 500,\n            'take_profit_multiplier': 1.0015,\n            'stop_loss_multiplier': 0.0005,\n            'quantity': '1000000',\n            'account_id': account_id\n        }\n    \n    def generate_signals(self, data):\n        config = self.config\n        \n        bearish_signals = data.apply(lambda x: 2 if (\n            x['bearish_2'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        bullish_signals = data.apply(lambda x: 1 if (\n            x['bullish_1'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n    \n    def get_order_parameters(self, current_price, signal):\n        config = self.config\n        \n        if signal == 2:  # Bearish\n            return {\n                'take_profit': current_price * config['take_profit_multiplier'],\n                'stop_loss': config['stop_loss_multiplier'],\n                'quantity': config['quantity'],\n                'type': 'BUY',\n                'account_id': config['account_id']\n            }\n        elif signal == 1:  # Bullish\n            return {\n                'take_profit': current_price * (1 - config['stop_loss_multiplier']),\n                'stop_loss': config['stop_loss_multiplier'],\n                'quantity': f\"-{config['quantity']}\",\n                'type': 'SELL',\n                'account_id': config['account_id']\n            }\n        return None\n\nclass EMASlopeStrategy(BaseStrategy):\n    def __init__(self):\n        super().__init__('ema_slope')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': -0.0002,\n            'ema_slope_15_threshold': -0.0001,\n            'ema_slope_60_threshold': 0,\n            'volume_mean_5_min': 450,\n            'volume_mean_5_max': 700,\n            'bearish_threshold_min': 0.5,\n            'bearish_threshold_max': 0.65,\n            'move_from_top_min': 0.08,\n            'move_from_top_max': 0.25,\n            'move_from_bottom_min': 0.08,\n            'move_from_bottom_max': 0.25,\n            'volume_min': 450,\n            'volume_max': 700,\n            'volume_mean_3_min': 500,\n            'volume_mean_3_max': 750,\n            'take_profit_multiplier': 1.002,\n            'stop_loss_multiplier': 0.001,\n            'quantity': '500000',\n            'account_id': account_id\n        }\n    \n    def generate_signals(self, data):\n        config = self.config\n        \n        bearish_signals = data.apply(lambda x: 2 if (\n            x['volume_mean_5'] > config['volume_mean_5_min'] and\n            x['volume_mean_5'] < config['volume_mean_5_max'] and\n            x['bearish_2'] > config['bearish_threshold_min'] and\n            x['bearish_2'] < config['bearish_threshold_max'] and\n            x['move_from_top'] > config['move_from_top_min'] and\n            x['move_from_top'] < config['move_from_top_max'] and\n            x['volume'] > config['volume_min'] and\n            x['volume'] < config['volume_max'] and\n            x['volume_mean_3'] > config['volume_mean_3_min'] and\n            x['volume_mean_3'] < config['volume_mean_3_max']\n        ) else 0, axis=1)\n\n        bullish_signals = data.apply(lambda x: 1 if (\n            x['volume'] >= config['volume_min'] and\n            x['volume'] <= config['volume_max'] and\n            x['volume_mean_3'] >= config['volume_mean_3_min'] and\n            x['volume_mean_3'] <= config['volume_mean_3_max'] and\n            x['volume_mean_5'] >= config['volume_mean_5_min'] and\n            x['volume_mean_5'] <= config['volume_mean_5_max'] and\n            x['bullish_1'] >= config['bearish_threshold_min'] and\n            x['bullish_1'] <= config['bearish_threshold_max'] and\n            x['move_from_bottom'] >= config['move_from_bottom_min'] and\n            x['move_from_bottom'] <= config['move_from_bottom_max']\n        ) else 0, axis=1)\n\n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n    \n    def get_order_parameters(self, current_price, signal):\n        config = self.config\n        \n        if signal == 1:  # Bullish\n            return {\n                'take_profit': current_price * config['take_profit_multiplier'],\n                'stop_loss': config['stop_loss_multiplier'],\n                'quantity': config['quantity'],\n                'type': 'BUY',\n                'account_id': config['account_id']\n            }\n        elif signal == 2:  # Bearish\n            return {\n                'take_profit': current_price * (1 - config['stop_loss_multiplier']),\n                'stop_loss': config['stop_loss_multiplier'],\n                'quantity': f\"-{config['quantity']}\",\n                'type': 'SELL',\n                'account_id': config['account_id']\n            }\n        return None",
      "block_group": "86288f70820f4128b1cfc4b18c880480",
      "execution_count": 12,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "216eebd8",
        "execution_start": 1743981631602,
        "execution_millis": 498,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "52b1e082631d41fca8a8dd78f26dad73",
        "deepnote_cell_type": "code"
      },
      "source": "# Initialize the system\ntrading_system = TradingSystem(access_token, bearish_model, bullish_model)\n\n# Register strategies\nvolume_strategy = VolumeBasedStrategy()\nvolume_strategy_hedge = VolumeBasedStrategyHedge()\nema_strategy = EMASlopeStrategy()\n\nStrategyRegistry.register(volume_strategy)\nStrategyRegistry.register(volume_strategy_hedge)\nStrategyRegistry.register(ema_strategy)",
      "block_group": "ad04ba6aad8847a2bbc3cc519f349dab",
      "execution_count": 14,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "1c5085c4",
        "is_code_hidden": true,
        "execution_start": 1743981635155,
        "execution_millis": 4,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_app_is_code_hidden": true,
        "cell_id": "6437b2eff5874155af722cb26f1d337c",
        "deepnote_cell_type": "code"
      },
      "source": "import os\n\n# Check if logs directory exists and create if not\nlog_dir = 'logs'\nif not os.path.exists(log_dir):\n    os.makedirs(log_dir)\n\n# Test if log file is being written\nimport logging\n\n# Configure logging with both file and console handlers\nlogger = logging.getLogger('TradingSystem')\nlogger.setLevel(logging.INFO)\n\n# Clear any existing handlers\nlogger.handlers = []\n\n# Create file handler\nfile_handler = logging.FileHandler('logs/trading_system.log')\nfile_handler.setLevel(logging.INFO)\nfile_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nfile_handler.setFormatter(file_formatter)\n\n# Create console handler\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel(logging.INFO)\nconsole_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nconsole_handler.setFormatter(console_formatter)\n\n# Add both handlers to logger\nlogger.addHandler(file_handler)\nlogger.addHandler(console_handler)\n\n# Test logging\nlogger.info(\"Test log message - checking if logging is working\")\n\n# Read and display the last few lines of the log file\ntry:\n    with open('logs/trading_system.log', 'r') as f:\n        last_lines = f.readlines()[-5:]  # Get last 5 lines\n        print(\"\\nLast few lines from log file:\")\n        for line in last_lines:\n            print(line.strip())\nexcept FileNotFoundError:\n    print(\"Log file not found!\")\nexcept Exception as e:\n    print(f\"Error reading log file: {str(e)}\")",
      "block_group": "7721ec0d405c4fb48da03c0d4d969f38",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-06 23:20:35,154 - INFO - Test log message - checking if logging is working\n\nLast few lines from log file:\n2025-04-06 23:20:32,095 - TradingSystem - INFO - Strategy registered: volume_based\n2025-04-06 23:20:32,096 - TradingSystem - INFO - Strategy registered: volume_based_hedge\n2025-04-06 23:20:32,096 - TradingSystem - INFO - Strategy registered: ema_slope\n2025-04-06 23:20:35,154 - TradingSystem - INFO - Test log message - checking if logging is working\n2025-04-06 23:20:35,154 - TradingSystem - INFO - Test log message - checking if logging is working\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "6d0ee7c5",
        "execution_start": 1743983424395,
        "execution_millis": 0,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "8d39940b102940aeb0c3f0af5595cba2",
        "deepnote_cell_type": "code"
      },
      "source": "def job():\n    trading_system.execute(lookback=100)",
      "block_group": "e7eb13f062ac44c891b1a1eef3ec8e2b",
      "execution_count": 44,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "5d116ed6",
        "execution_start": 1743981640314,
        "execution_millis": 0,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "e38c3775cbce4c98bd5308a3decf99f0",
        "deepnote_cell_type": "code"
      },
      "source": "# Set up the schedule to run at specified intervals\nfor minute in [1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56]:\n    schedule.every().hour.at(f\":{minute:02d}\").do(job)",
      "block_group": "f26de8f42aeb4038a9c514548e1604a1",
      "execution_count": 20,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "9db07433",
        "execution_start": 1743983877205,
        "execution_millis": 185291,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "e2b75dd7944840a6aec43f6b1881fad2",
        "deepnote_cell_type": "code"
      },
      "source": "print(\"Starting the trading bot...\")\nwhile True:\n    schedule.run_pending()\n    time.sleep(1)",
      "block_group": "3472ca3e039440d184c51551be756c4a",
      "execution_count": 50,
      "outputs": [
        {
          "name": "stdout",
          "text": "Starting the trading bot...\n2025-04-06 23:57:57,430 - INFO - Raw data saved to data/raw/base_data.csv\n2025-04-06 23:57:58,019 - INFO - Processed data saved to data/processed/transformed_data.csv\n2025-04-06 23:57:58,433 - INFO - Signals saved for strategy volume_based\n2025-04-06 23:57:58,524 - INFO - Signals saved for strategy volume_based_hedge\n2025-04-06 23:57:58,630 - INFO - Signals saved for strategy ema_slope\n2025-04-06 23:57:58,632 - INFO - Execution completed at 2025-04-06 23:57:58.631984\n2025-04-07 00:01:01,659 - INFO - Raw data saved to data/raw/base_data.csv\n2025-04-07 00:01:02,253 - INFO - Processed data saved to data/processed/transformed_data.csv\n2025-04-07 00:01:02,404 - ERROR - Error in execute: Reindexing only valid with uniquely valued Index objects\n",
          "output_type": "stream"
        },
        {
          "output_type": "error",
          "ename": "InvalidIndexError",
          "evalue": "Reindexing only valid with uniquely valued Index objects",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting the trading bot...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mschedule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pending\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/schedule/__init__.py:854\u001b[0m, in \u001b[0;36mrun_pending\u001b[0;34m()\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_pending\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls :meth:`run_pending <Scheduler.run_pending>` on the\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m    :data:`default scheduler instance <default_scheduler>`.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m     \u001b[43mdefault_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pending\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/schedule/__init__.py:101\u001b[0m, in \u001b[0;36mScheduler.run_pending\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m runnable_jobs \u001b[38;5;241m=\u001b[39m (job \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs \u001b[38;5;28;01mif\u001b[39;00m job\u001b[38;5;241m.\u001b[39mshould_run)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(runnable_jobs):\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/schedule/__init__.py:173\u001b[0m, in \u001b[0;36mScheduler._run_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, CancelJob) \u001b[38;5;129;01mor\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m CancelJob:\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_job(job)\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/schedule/__init__.py:691\u001b[0m, in \u001b[0;36mJob.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CancelJob\n\u001b[1;32m    690\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning job \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 691\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_run \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedule_next_run()\n",
            "Cell \u001b[0;32mIn[44], line 2\u001b[0m, in \u001b[0;36mjob\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjob\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrading_system\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlookback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 22\u001b[0m, in \u001b[0;36mTradingSystem.execute\u001b[0;34m(self, lookback)\u001b[0m\n\u001b[1;32m     18\u001b[0m y_pred_bullish \u001b[38;5;241m=\u001b[39m bullish_model\u001b[38;5;241m.\u001b[39mpredict(X_normalized, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#y_pred_bullish = pd.DataFrame(y_pred_bullish)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Combine results with datetime from base_data\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m result_combined \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_bearish\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbearish_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbearish_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbearish_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbearish_3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred_bullish\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbullish_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbullish_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbullish_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbullish_3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Read existing predictions history\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     existing_preds \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictions_data_path)\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:393\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    378\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    380\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    381\u001b[0m     objs,\n\u001b[1;32m    382\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    391\u001b[0m )\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/pandas/core/reshape/concat.py:676\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    674\u001b[0m         obj_labels \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m ax]\n\u001b[1;32m    675\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_labels\u001b[38;5;241m.\u001b[39mequals(obj_labels):\n\u001b[0;32m--> 676\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m \u001b[43mobj_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m    680\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[1;32m    681\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    682\u001b[0m )\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3875\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3872\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 3875\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requires_unique_msg)\n\u001b[1;32m   3877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n",
            "\u001b[0;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
          ]
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "e0bc1aeacb344758b9e335baa1275f46",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "block_group": "8e62b09bac4c42bb935d290d898dc867",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a127aa9e-0a77-4af9-a6ce-85e7a9b74042' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "11990d6ad3584e009dbd2e0e8ac5dabf"
  }
}