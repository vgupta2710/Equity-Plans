{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "545f3b74",
        "execution_start": 1743975521769,
        "execution_millis": 5347,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "051f8377b8c247a4941c80e87efdd609",
        "deepnote_cell_type": "code"
      },
      "source": "from Moduled_functions import get_data\nfrom Moduled_functions import tranformation\nfrom Moduled_functions import calculate_ema_slope\nfrom Moduled_functions import place_order\nfrom m_email import send_email\nimport pandas as pd\nfrom keras.models import load_model\nfrom sklearn.preprocessing import StandardScaler",
      "block_group": "e011e7f7e67449c1b21244e5d1d0acb8",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-06 21:38:45.311824: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-04-06 21:38:45.315411: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-04-06 21:38:45.346526: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-06 21:38:45.346592: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-06 21:38:45.347562: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-04-06 21:38:45.352847: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n2025-04-06 21:38:45.353322: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-04-06 21:38:46.141090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "51bab0e",
        "execution_start": 1743940717097,
        "execution_millis": 784,
        "is_output_hidden": true,
        "execution_context_id": "a31ff040-3a41-4c9a-9ac7-a98f94f552b1",
        "deepnote_app_is_output_hidden": true,
        "cell_id": "00f21d7de5bc487fb721a1659115a7bc",
        "deepnote_cell_type": "code"
      },
      "source": "import os\n\n# Create directory structure\ndef create_directory_structure():\n    directories = [\n        'data',\n        'data/raw',\n        'data/processed',\n        'data/signals',\n        'data/orders',\n        'data/predictions',\n        'logs',\n        'config'\n    ]\n    \n    for directory in directories:\n        os.makedirs(directory, exist_ok=True)\n        print(f\"Created directory: {directory}\")\n\ncreate_directory_structure()",
      "block_group": "00f21d7de5bc487fb721a1659115a7bc",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": "Created directory: data\nCreated directory: data/raw\nCreated directory: data/processed\nCreated directory: data/signals\nCreated directory: data/orders\nCreated directory: data/predictions\nCreated directory: models\nCreated directory: logs\nCreated directory: config\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "f2248899",
        "execution_start": 1743975527175,
        "execution_millis": 0,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "adb1ed85b389463e88051db3f863e8a1",
        "deepnote_cell_type": "code"
      },
      "source": "# Combined trading system framework\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport requests\nfrom abc import ABC, abstractmethod\nimport logging\nimport json\nfrom Moduled_functions import get_data\n\n# Configure logging\nlogging.basicConfig(\n    filename='logs/trading_system.log',\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger('TradingSystem')\nclass BaseStrategy(ABC):\n    def __init__(self, name):\n        self.name = name\n        self.signals_path = f'data/signals/signals_{name}.csv'\n        self.trades_path = f'data/orders/trades_{name}.csv'\n        self.config_path = f'config/strategy_{name}.json'\n        self.load_config()\n    \n    def load_config(self):\n        \"\"\"Load strategy configuration\"\"\"\n        try:\n            with open(self.config_path, 'r') as f:\n                self.config = json.load(f)\n        except FileNotFoundError:\n            self.config = self.get_default_config()\n            self.save_config()\n    \n    def save_config(self):\n        \"\"\"Save strategy configuration\"\"\"\n        with open(self.config_path, 'w') as f:\n            json.dump(self.config, f, indent=4)\n    \n    @abstractmethod\n    def get_default_config(self):\n        \"\"\"Get default strategy configuration\"\"\"\n        pass\n    \n    @abstractmethod\n    def generate_signals(self, data):\n        \"\"\"Generate trading signals based on strategy logic\"\"\"\n        pass\n    \n    @abstractmethod\n    def get_order_parameters(self, current_price, signal):\n        \"\"\"Get order parameters based on strategy signal\"\"\"\n        pass\n    \n    def save_signals(self, signals_data):\n        \"\"\"Save strategy signals to history\"\"\"\n        try:\n            if os.path.exists(self.signals_path):\n                existing_signals = pd.read_csv(self.signals_path)\n                signals_data = signals_data.iloc[-1:]\n                updated_signals = pd.concat([existing_signals, signals_data])\n            else:\n                updated_signals = signals_data\n            updated_signals.to_csv(self.signals_path, index=False)\n            logger.info(f\"Signals saved for strategy {self.name}\")\n        except Exception as e:\n            logger.error(f\"Error saving signals for strategy {self.name}: {str(e)}\")\n    \n    def save_trade(self, trade_data):\n        \"\"\"Save trade details to history\"\"\"\n        try:\n            if os.path.exists(self.trades_path):\n                existing_trades = pd.read_csv(self.trades_path)\n                updated_trades = pd.concat([existing_trades, trade_data])\n            else:\n                updated_trades = trade_data\n            updated_trades.to_csv(self.trades_path, index=False)\n            logger.info(f\"Trade saved for strategy {self.name}\")\n        except Exception as e:\n            logger.error(f\"Error saving trade for strategy {self.name}: {str(e)}\")\n\nclass StrategyRegistry:\n    _strategies = {}\n    \n    @classmethod\n    def register(cls, strategy_instance):\n        \"\"\"Register a new strategy\"\"\"\n        cls._strategies[strategy_instance.name] = strategy_instance\n        logger.info(f\"Strategy registered: {strategy_instance.name}\")\n    \n    @classmethod\n    def get_strategy(cls, name):\n        \"\"\"Get a registered strategy by name\"\"\"\n        return cls._strategies.get(name)\n    \n    @classmethod\n    def get_all_strategies(cls):\n        \"\"\"Get all registered strategies\"\"\"\n        return cls._strategies",
      "block_group": "d004a156fdd64a0a98947cf34419eae7",
      "execution_count": 2,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "ade30329",
        "execution_start": 1743977569795,
        "execution_millis": 0,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "504fb6ab52404f93a5301b732be9f788",
        "deepnote_cell_type": "code"
      },
      "source": "class DataHandler:\n    def __init__(self):\n        self.scaler = None\n        self.raw_data_path = 'data/raw/base_data.csv'\n        self.processed_data_path = 'data/processed/transformed_data.csv'\n    \n    def get_and_prepare_data(self, lookback, access_token):\n        from Moduled_functions import tranformation\n        \"\"\"Get and prepare the base data\"\"\"\n        try:\n            base_data, current_price = get_data(lookback, access_token)\n            #base_data_append=base_data.iloc[-1:]\n\n        # Read existing history\n            try:\n                existing_data = pd.read_csv(self.raw_data_path)\n                existing_data['datetime'] = pd.to_datetime(existing_data['datetime'])\n                base_data['datetime'] = pd.to_datetime(base_data['datetime'])\n                \n                \n                # Add new records\n                new_records = base_data[~base_data['datetime'].isin(existing_data['datetime'])]\n                if not new_records.empty:\n                    updated_data = pd.concat([existing_data, new_records])\n                    updated_data = updated_data.sort_values('datetime')\n                else:\n                    updated_data = existing_data\n            except FileNotFoundError:\n                updated_data = base_data\n\n            updated_data.to_csv(self.raw_data_path, index=False)\n            logger.info(f\"Raw data saved to {self.raw_data_path}\")\n            \n            base_data = tranformation(base_data, 0.0015)            \n            return base_data, current_price\n        except Exception as e:\n            logger.error(f\"Error in get_and_prepare_data: {str(e)}\")\n            raise\n    \n    def calculate_technical_indicators(self, data):\n        \"\"\"Calculate technical indicators\"\"\"\n        try:\n            data['EMA_slope'] = calculate_ema_slope(data, 'Open', 9)\n            data['EMA_slope_15'] = calculate_ema_slope(data, 'Open', 15)\n            data['EMA_slope_60'] = calculate_ema_slope(data, 'Open', 60)\n            data['deviation_'] = data['bullish_move_flag_20'] + data['bearish_move_flag_20']\n            \n            try:\n                # Read existing history\n                existing_data = pd.read_csv(self.processed_data_path)\n                existing_data['datetime'] = pd.to_datetime(existing_data['datetime'])\n                data['datetime'] = pd.to_datetime(data['datetime'])\n                # Add new records\n                new_records = data[~data['datetime'].isin(existing_data['datetime'])]\n                if not new_records.empty:\n                    data = pd.concat([existing_data, new_records])\n                    data = data.sort_values('datetime')\n                else:\n                    data = existing_data\n            except FileNotFoundError:\n                data.to_csv(self.processed_data_path, index=False)\n\n            data.to_csv(self.processed_data_path, index=False)\n\n            logger.info(f\"Processed data saved to {self.processed_data_path}\")\n            return data\n        except Exception as e:\n            logger.error(f\"Error in calculate_technical_indicators: {str(e)}\")\n            raise\n    \n    def normalize_features(self, data):\n        \"\"\"Normalize the feature data\"\"\"\n        try:\n            X = data.iloc[:,9:69]\n            y = data['deviation_']\n            \n            if self.scaler is None:\n                self.scaler = StandardScaler()\n                self.scaler.fit(X)\n            \n            X_normalized = pd.DataFrame(self.scaler.transform(X)).round(1)\n            return X_normalized, y\n        except Exception as e:\n            logger.error(f\"Error in normalize_features: {str(e)}\")\n            raise\nclass OrderManager:\n    def __init__(self, access_token):\n        self.access_token = access_token\n        #self.account_id = account_id\n        self.orders_path = 'data/orders/all_orders.csv'\n    \n    def place_order(self, current_price, stop_loss, take_profit, quantity,account_id):\n        \"\"\"Place an order\"\"\"\n        try:\n            order = place_order(\n                str(current_price),\n                str(round(stop_loss, 5)),\n                str(round(take_profit, 5)),\n                quantity,\n                self.access_token,\n                account_id\n            )\n            logger.info(f\"Order placed: {order['orderCreateTransaction'].get('id')}\")\n            return order\n        except Exception as e:\n            logger.error(f\"Error placing order: {str(e)}\")\n            raise\n    \n    def save_order(self, order_data):\n        \"\"\"Save order details to history\"\"\"\n        try:\n            if os.path.exists(self.orders_path):\n                existing_orders = pd.read_csv(self.orders_path)\n                updated_orders = pd.concat([existing_orders, order_data])\n            else:\n                updated_orders = order_data\n            updated_orders.to_csv(self.orders_path, index=False)\n            logger.info(\"Order saved to history\")\n        except Exception as e:\n            logger.error(f\"Error saving order: {str(e)}\")",
      "block_group": "98e7de400ce44087bb93324ee96e7f8e",
      "execution_count": 43,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "ca0cead3",
        "execution_start": 1743977571354,
        "execution_millis": 0,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "eb4c0d0fba4740ee82a9e6bcc4475405",
        "deepnote_cell_type": "code"
      },
      "source": "class TradingSystem:\n    def __init__(self, access_token, bearish_model, bullish_model):\n        self.data_handler = DataHandler()\n        self.order_manager = OrderManager(access_token)\n        self.predictions_data_path = 'data/predictions/predictions_data.csv'\n        #self.bearish_model = bearish_model\n        #self.bullish_model = bullish_model\n        \n    def execute(self, lookback=100):\n        try:\n            # Get and prepare data\n            base_data, current_price = self.data_handler.get_and_prepare_data(lookback, self.order_manager.access_token)\n            base_data = self.data_handler.calculate_technical_indicators(base_data)\n            X_normalized, y = self.data_handler.normalize_features(base_data)\n            \n            # Generate predictions\n            \n            y_pred_bearish = bearish_model.predict(X_normalized, verbose=0)\n            y_pred_bearish = pd.DataFrame(y_pred_bearish)\n            y_pred_bullish = bullish_model.predict(X_normalized, verbose=0)\n            y_pred_bullish = pd.DataFrame(y_pred_bullish)\n            \n            # Combine results\n            result_combined = pd.concat([y_pred_bearish, y_pred_bullish, y], axis=1)\n            result_combined.columns = ['bearish_0','bearish_1','bearish_2','bearish_3',\n                                    'bullish_0','bullish_1','bullish_2','bullish_3','y_true']\n            \n            result_combined = pd.concat([base_data['datetime'], result_combined], axis=1)\n\n            try:\n                # Read existing history\n                existing_data = pd.read_csv(self.predictions_data_path)\n                existing_data['datetime'] = pd.to_datetime(existing_data['datetime'])\n                result_combined['datetime'] = pd.to_datetime(result_combined['datetime'])\n                \n                \n                # Add new records\n                new_records = result_combined[~result_combined['datetime'].isin(existing_data['datetime'])]\n                if not new_records.empty:\n                    updated_data = pd.concat([existing_data, new_records])\n                    updated_data = updated_data.sort_values('datetime')\n                else:\n                    updated_data = existing_data\n            except FileNotFoundError:\n                updated_data = result_combined\n                \n            updated_data.to_csv(self.predictions_data_path, index=False)\n\n            del(result_combined)\n            # Process each strategy\n            for strategy in StrategyRegistry.get_all_strategies().values():\n                self._process_strategy(strategy, base_data, current_price)\n            \n            logger.info(f\"Execution completed at {datetime.now()}\")\n            \n        except Exception as e:\n            logger.error(f\"Error in execute: {str(e)}\")\n    \n    def _process_strategy(self, strategy, base_data, current_price):\n        \"\"\"Process individual strategy signals and execute trades\"\"\"\n        try:\n            # Generate signals\n            signals = strategy.generate_signals(base_data)\n            signals['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            strategy.save_signals(signals)\n            \n            # Check signals and execute trades\n            latest_bearish = signals['bearish_flag'].iloc[-1]\n            latest_bullish = signals['bullish_flag'].iloc[-1]\n            \n            if latest_bullish == 1 and latest_bearish != 2:\n                self._execute_trade(strategy, 1, current_price)\n            elif latest_bearish == 2 and latest_bullish != 1:\n                self._execute_trade(strategy, 2, current_price)\n                \n        except Exception as e:\n            logger.error(f\"Error processing strategy {strategy.name}: {str(e)}\")\n    \n    def _execute_trade(self, strategy, signal, current_price):\n        \"\"\"Execute and record a trade for a strategy\"\"\"\n        try:\n            order_params = strategy.get_order_parameters(current_price, signal)\n            if not order_params:\n                return\n            \n            order = self.order_manager.place_order(\n                current_price,\n                order_params['stop_loss'],\n                order_params['take_profit'],\n                order_params['quantity'],\n                order_params['account_id']\n            )\n            \n            order_id = order['orderCreateTransaction'].get('id')\n            order_time = order['orderCreateTransaction'].get('time')\n            account_id = order['orderCreateTransaction'].get('accountID')\n            \n            # Save trade details\n            trade_data = pd.DataFrame({\n                'Time': [order_time],\n                'Strategy': [strategy.name],\n                'Type': [order_params['type']],\n                'Quantity': [order_params['quantity']],\n                'Price': [current_price],\n                'Take_Profit': [order_params['take_profit']],\n                'Stop_Loss': [order_params['stop_loss']],\n                'Order_ID': [order_id],\n                'Account_ID': [account_id],\n            })\n            \n            strategy.save_trade(trade_data)\n            self.order_manager.save_order(trade_data)\n            \n            # Send notification\n            message = f\"{strategy.name} {order_params['type']} Order placed with order no: {order_id}\"\n            requests.get(f'https://api.day.app/iFbt9PqBdm6d2YvUT4irnN/{order_params[\"type\"]} Order Placed/{message}')\n            logger.info(message)\n            \n        except Exception as e:\n            logger.error(f\"Error executing trade for strategy {strategy.name}: {str(e)}\")\n\nprint(\"Trading system framework initialized with directory structure\")",
      "block_group": "dc265a2ae0a043018ce77028bce37b98",
      "execution_count": 45,
      "outputs": [
        {
          "name": "stdout",
          "text": "Trading system framework initialized with directory structure\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "6817e492",
        "execution_start": 1743977730325,
        "execution_millis": 0,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "8e1f6e96a5094d3992fb9e5d9fa5839b",
        "deepnote_cell_type": "code"
      },
      "source": "class VolumeBasedStrategy(BaseStrategy):\n    def __init__(self):\n        super().__init__('volume_based')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': 450,\n            'ema_slope_15_threshold': 750,\n            'ema_slope_60_threshold': 0.50,\n            'bearish_threshold_max': 0.65,\n            'move_from_top_min': 0.08,\n            'move_from_top_max': 0.25,\n            'volume_min': 450,\n            'volume_max': 700,\n            'volume_mean_3_min': 500,\n            'volume_mean_3_max': 750,\n            'take_profit_multiplier': 1.0015,\n            'stop_loss_multiplier': 0.0005,\n            'quantity': '1000000'\n        }\n    \n    def generate_signals(self, data):\n        predictions = pd.read_csv('data/predictions/predictions_data.csv')\n        predictions['datetime'] = pd.to_datetime(predictions['datetime'])\n        data['datetime'] = pd.to_datetime(data['datetime'])\n        \n        # Merge the predictions with the input data\n        data = pd.merge(data, predictions, on='datetime', how='inner')\n        \n        config = self.config\n        \n        # Generate bearish signals\n        bearish_signals = data.apply(lambda x: 2 if (\n            x['bearish_2'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] |\n            x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        # Generate bullish signals\n        bullish_signals = data.apply(lambda x: 1 if (\n            x['bullish_1'] > config['bullish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] |\n            x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n        \n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n    \n    def get_order_parameters(self, current_price, signal):\n        config = self.config\n        \n        if signal == 1:  # Bullish\n            return {\n                'take_profit': current_price * config['take_profit_multiplier'],\n                'stop_loss': current_price * config['stop_loss_multiplier'],\n                'quantity': config['quantity'],\n                'type': 'BUY',\n                'account_id' : config['account_id']\n            }\n        elif signal == 2:  # Bearish\n            return {\n                'take_profit': current_price * (2 - config['take_profit_multiplier']),\n                'stop_loss': current_price * config['stop_loss_multiplier'],\n                'quantity': f\"-{config['quantity']}\",\n                'type': 'SELL',\n                'account_id' : config['account_id']\n            }\n        return None\n\nclass VolumeBasedStrategyHedge(BaseStrategy):\n    def __init__(self):\n        super().__init__('volume_based_hedge')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': 450,\n            'ema_slope_15_threshold': 750,\n            'ema_slope_60_threshold': 0.50,\n            'bearish_threshold_max': 0.65,\n            'move_from_top_min': 0.08,\n            'move_from_top_max': 0.25,\n            'volume_min': 450,\n            'volume_max': 700,\n            'volume_mean_3_min': 500,\n            'volume_mean_3_max': 750,\n            'take_profit_multiplier': 1.0015,\n            'stop_loss_multiplier': 0.0005,\n            'quantity': '1000000'\n        }\n    \n    def generate_signals(self, data):\n        predictions = pd.read_csv('data/predictions/predictions_data.csv')\n        predictions['datetime'] = pd.to_datetime(predictions['datetime'])\n        data['datetime'] = pd.to_datetime(data['datetime'])\n        \n        # Merge the predictions with the input data\n        data = pd.merge(data, predictions, on='datetime', how='inner')\n        config = self.config\n        \n        # Generate bearish signals\n        bearish_signals = data.apply(lambda x: 2 if (\n            x['bearish_2'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] |\n            x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        # Generate bullish signals\n        bullish_signals = data.apply(lambda x: 1 if (\n            x['bullish_1'] > config['bullish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] |\n            x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n    \n    def get_order_parameters(self, current_price, signal):\n        config = self.config\n        \n        if signal == 2:  #Bearish \n            return {\n                'take_profit': current_price * config['take_profit_multiplier'],\n                'stop_loss': current_price * config['stop_loss_multiplier'],\n                'quantity': config['quantity'],\n                'type': 'BUY',\n                'account_id' : config['account_id']\n            }\n        elif signal == 1:  # Bullish\n            return {\n                'take_profit': current_price * (2 - config['take_profit_multiplier']),\n                'stop_loss': current_price * config['stop_loss_multiplier'],\n                'quantity': f\"-{config['quantity']}\",\n                'type': 'BUY',\n                'account_id' : config['account_id']\n            }\n        return None\n\nclass EMASlopeStrategy(BaseStrategy):\n    def __init__(self):\n        super().__init__('ema_slope')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': -0.0002,\n            'ema_slope_15_threshold': -0.0001,\n            'ema_slope_60_threshold': 0,\n            'volume_threshold': 500,\n            'take_profit_multiplier': 1.002,\n            'stop_loss_multiplier': 0.001,\n            'quantity': '500000'\n        }\n    \n    def generate_signals(self, data):\n        predictions = pd.read_csv('data/predictions/predictions_data.csv')\n        predictions['datetime'] = pd.to_datetime(predictions['datetime'])\n        data['datetime'] = pd.to_datetime(data['datetime'])\n        \n        # Merge the predictions with the input data\n        data = pd.merge(data, predictions, on='datetime', how='inner')\n        config = self.config\n        \n        # Generate bearish signals\n\n        bearish_signals = data.apply(lambda x:2 if x['volume_mean_5'] > config['volume_mean_5_min']\n                                                and x['volume_mean_5'] <config['volume_mean_5_max']\n                                                and x['bearish_2'] > bearish_threshold_min \n                                                and x['bearish_2'] < bearish_threshold_max \n                                                and x['move_from_top'] > move_from_top_min\n                                                and x['move_from_top'] < move_from_top_max\n                                                and x['volume'] > volume_min\n                                                and x['volume'] <volume_max\n                                                and x['volume_mean_3'] > volume_mean_3_min\n                                                and x['volume_mean_3'] <volume_mean_3_max else 0, axis=1)\n        ## bullish indicator\n        bullish_signals = data.apply(lambda x:1 if x['volume'] >= volume_min\n                                                and x['volume'] <= volume_max\n                                                and x['volume_mean_3'] >= volume_mean_3_min\n                                                and x['volume_mean_3'] <= volume_mean_3_max\n                                                and x['volume_mean_5'] >= volume_mean_5_min\n                                                and x['volume_mean_5'] <= volume_mean_5_max\n                                                and x['bullish_1'] >= bearish_threshold_min\n                                                and x['bullish_1'] <= bearish_threshold_max\n                                                and x['move_from_bottom'] >= move_from_bottom_min\n                                                and x['move_from_bottom'] <= move_from_bottom_max else 0,axis=1)\n        \n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n    \n    def get_order_parameters(self, current_price, signal):\n        config = self.config\n        \n        if signal == 1:  # Bullish\n            return {\n                'take_profit': current_price * config['take_profit_multiplier'],\n                'stop_loss': current_price * config['stop_loss_multiplier'],\n                'quantity': config['quantity'],\n                'type': 'BUY',\n                'account_id' : config['account_id']\n            }\n        elif signal == 2:  # Bearish\n            return {\n                'take_profit': current_price * (2 - config['take_profit_multiplier']),\n                'stop_loss': current_price * config['stop_loss_multiplier'],\n                'quantity': f\"-{config['quantity']}\",\n                'type': 'SELL',\n                'account_id' : config['account_id']\n            }\n        return None\n\nprint(\"Strategy implementations completed\")",
      "block_group": "86288f70820f4128b1cfc4b18c880480",
      "execution_count": 49,
      "outputs": [
        {
          "name": "stdout",
          "text": "Strategy implementations completed\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "172532c1",
        "execution_start": 1743977735055,
        "execution_millis": 1530,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "52b1e082631d41fca8a8dd78f26dad73",
        "deepnote_cell_type": "code"
      },
      "source": "access_token = '320c03e29b3723c3869a2eafa278c553-c96eccee26e4e5864dcf001e98f7db84'\naccount_id = '101-004-31059296-001'\nbearish_model = load_model('/work/Trained_Models/FX_Bearish_model_2025-02-19.keras')\nbullish_model = load_model('/work/Trained_Models/FX_Bullish_model_2025-02-21.keras')\n\n\n# Initialize the system\ntrading_system = TradingSystem(access_token, bearish_model, bullish_model)\n\n# Register strategies\nvolume_strategy = VolumeBasedStrategy()\nvolume_strategy_hedge = VolumeBasedStrategyHedge()\nema_strategy = EMASlopeStrategy()\n\nStrategyRegistry.register(volume_strategy)\nStrategyRegistry.register(volume_strategy_hedge)\nStrategyRegistry.register(ema_strategy)",
      "block_group": "ad04ba6aad8847a2bbc3cc519f349dab",
      "execution_count": 51,
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-06 22:15:36,525 - INFO - Strategy registered: volume_based\n2025-04-06 22:15:36,526 - INFO - Strategy registered: volume_based_hedge\n2025-04-06 22:15:36,527 - INFO - Strategy registered: ema_slope\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "1c5085c4",
        "is_code_hidden": true,
        "execution_start": 1743975638785,
        "execution_millis": 81,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_app_is_code_hidden": true,
        "cell_id": "6437b2eff5874155af722cb26f1d337c",
        "deepnote_cell_type": "code"
      },
      "source": "import os\n\n# Check if logs directory exists and create if not\nlog_dir = 'logs'\nif not os.path.exists(log_dir):\n    os.makedirs(log_dir)\n\n# Test if log file is being written\nimport logging\n\n# Configure logging with both file and console handlers\nlogger = logging.getLogger('TradingSystem')\nlogger.setLevel(logging.INFO)\n\n# Clear any existing handlers\nlogger.handlers = []\n\n# Create file handler\nfile_handler = logging.FileHandler('logs/trading_system.log')\nfile_handler.setLevel(logging.INFO)\nfile_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nfile_handler.setFormatter(file_formatter)\n\n# Create console handler\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel(logging.INFO)\nconsole_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nconsole_handler.setFormatter(console_formatter)\n\n# Add both handlers to logger\nlogger.addHandler(file_handler)\nlogger.addHandler(console_handler)\n\n# Test logging\nlogger.info(\"Test log message - checking if logging is working\")\n\n# Read and display the last few lines of the log file\ntry:\n    with open('logs/trading_system.log', 'r') as f:\n        last_lines = f.readlines()[-5:]  # Get last 5 lines\n        print(\"\\nLast few lines from log file:\")\n        for line in last_lines:\n            print(line.strip())\nexcept FileNotFoundError:\n    print(\"Log file not found!\")\nexcept Exception as e:\n    print(f\"Error reading log file: {str(e)}\")",
      "block_group": "7721ec0d405c4fb48da03c0d4d969f38",
      "execution_count": 17,
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-06 21:40:38,861 - INFO - Test log message - checking if logging is working\n\nLast few lines from log file:\n2025-04-06 21:39:40,784 - TradingSystem - INFO - Test log message - checking if logging is working\n2025-04-06 21:39:40,784 - TradingSystem - INFO - Test log message - checking if logging is working\n2025-04-06 21:40:38,861 - TradingSystem - INFO - Test log message - checking if logging is working\n2025-04-06 21:40:38,861 - TradingSystem - INFO - Test log message - checking if logging is working\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "935e3a71",
        "execution_start": 1743977745888,
        "execution_millis": 1374,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "1cc4ff1941ab42a790378ea8e70c1808",
        "deepnote_cell_type": "code"
      },
      "source": "trading_system.execute(lookback=100)",
      "block_group": "979b15a7705c48d494a6f5b1ae5deb94",
      "execution_count": 53,
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-06 22:15:46,416 - INFO - Raw data saved to data/raw/base_data.csv\n2025-04-06 22:15:46,962 - INFO - Processed data saved to data/processed/transformed_data.csv\n2025-04-06 22:15:47,256 - ERROR - Error in execute: Reindexing only valid with uniquely valued Index objects\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "e15a9ffb",
        "execution_start": 1743975614575,
        "execution_millis": 0,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "8d39940b102940aeb0c3f0af5595cba2",
        "deepnote_cell_type": "code"
      },
      "source": "def job():\n    # Execute trading system\n    trading_system.execute(lookback=100)\n    current_time = datetime.now()\n    print(f\"Check completed at {current_time}\")",
      "block_group": "e7eb13f062ac44c891b1a1eef3ec8e2b",
      "execution_count": 13,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "92339867",
        "execution_start": 1743975640796,
        "execution_millis": 20108,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "e38c3775cbce4c98bd5308a3decf99f0",
        "deepnote_cell_type": "code"
      },
      "source": "import schedule\nimport time\nfrom datetime import datetime\n\n# Set up the schedule to run at specified intervals\nfor minute in [1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51, 56]:\n    schedule.every().hour.at(f\":{minute:02d}\").do(job)",
      "block_group": "f26de8f42aeb4038a9c514548e1604a1",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trading_system' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     current_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mschedule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pending\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/schedule/__init__.py:854\u001b[0m, in \u001b[0;36mrun_pending\u001b[0;34m()\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_pending\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls :meth:`run_pending <Scheduler.run_pending>` on the\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m    :data:`default scheduler instance <default_scheduler>`.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m     \u001b[43mdefault_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pending\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/schedule/__init__.py:101\u001b[0m, in \u001b[0;36mScheduler.run_pending\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m runnable_jobs \u001b[38;5;241m=\u001b[39m (job \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs \u001b[38;5;28;01mif\u001b[39;00m job\u001b[38;5;241m.\u001b[39mshould_run)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(runnable_jobs):\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/schedule/__init__.py:173\u001b[0m, in \u001b[0;36mScheduler._run_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, CancelJob) \u001b[38;5;129;01mor\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m CancelJob:\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_job(job)\n",
            "File \u001b[0;32m~/venv/lib/python3.10/site-packages/schedule/__init__.py:691\u001b[0m, in \u001b[0;36mJob.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CancelJob\n\u001b[1;32m    690\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning job \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 691\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_run \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedule_next_run()\n",
            "Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mjob\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mjob\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Execute trading system\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrading_system\u001b[49m\u001b[38;5;241m.\u001b[39mexecute(lookback\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck completed at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trading_system' is not defined"
          ]
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "9db07433",
        "execution_start": 1743975915142,
        "execution_millis": 85100,
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "cell_id": "e2b75dd7944840a6aec43f6b1881fad2",
        "deepnote_cell_type": "code"
      },
      "source": "print(\"Starting the trading bot...\")\nwhile True:\n    schedule.run_pending()\n    time.sleep(1)",
      "block_group": "3472ca3e039440d184c51551be756c4a",
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "text": "Starting the trading bot...\n2025-04-06 21:46:00,744 - INFO - Raw data saved to data/raw/base_data.csv\n2025-04-06 21:46:01,297 - INFO - Processed data saved to data/processed/transformed_data.csv\n2025-04-06 21:46:01,764 - ERROR - Error processing strategy volume_based: 'bearish_2'\n2025-04-06 21:46:01,768 - ERROR - Error processing strategy volume_based_hedge: 'bearish_2'\n2025-04-06 21:46:01,772 - ERROR - Error processing strategy ema_slope: 'bearish_2'\n2025-04-06 21:46:01,773 - INFO - Execution completed at 2025-04-06 21:46:01.773743\nCheck completed at 2025-04-06 21:42:02.888252\n",
          "output_type": "stream"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     schedule\u001b[38;5;241m.\u001b[39mrun_pending()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "color": "purple",
        "cell_id": "9486b0ca763f44e8a0743c3806dc07cc",
        "deepnote_cell_type": "text-cell-callout"
      },
      "source": "> ERROR - Error in execute: Reindexing only valid with uniquely valued Index object.. ",
      "block_group": "2d99d533420a433fb7e56ca97312b59f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c18508fdcbcb43c69bd1cdbed877aad9",
        "deepnote_cell_type": "text-cell-p"
      },
      "source": "Let me examine the data and fix the error. The error suggests there are duplicate indices when trying to merge or concatenate dataframes. Let's check the data structure and duplicates.",
      "block_group": "567b8f91918f4a86b50c84b099a23c19"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "98ceebe3",
        "execution_start": 1743977785545,
        "execution_millis": 1,
        "sql_integration_id": "",
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_variable_name": "",
        "cell_id": "84b65bdab5e6405b930ee856d32f484c",
        "deepnote_cell_type": "code"
      },
      "source": "# Check for duplicate datetime values in df and predictions_df\nprint(\"Checking df for duplicate datetimes:\")\nprint(df['datetime'].value_counts()[df['datetime'].value_counts() > 1])\n\nprint(\"\\nChecking predictions_df for duplicate datetimes:\")\nprint(predictions_df['datetime'].value_counts()[predictions_df['datetime'].value_counts() > 1])\n\n# Show structure of both dataframes\nprint(\"\\ndf structure:\")\nprint(df.info())\n\nprint(\"\\npredictions_df structure:\")\nprint(predictions_df.info())",
      "block_group": "429c0ada002a436b867f4cd451237e53",
      "execution_count": 55,
      "outputs": [
        {
          "name": "stdout",
          "text": "Checking df for duplicate datetimes:\nSeries([], Name: count, dtype: int64)\n\nChecking predictions_df for duplicate datetimes:\nSeries([], Name: count, dtype: int64)\n\ndf structure:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 509 entries, 0 to 508\nData columns (total 91 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   datetime              509 non-null    object \n 1   Open                  509 non-null    float64\n 2   High                  509 non-null    float64\n 3   Low                   509 non-null    float64\n 4   Close                 509 non-null    float64\n 5   volume                509 non-null    int64  \n 6   minute                509 non-null    int64  \n 7   hour                  509 non-null    int64  \n 8   diff                  509 non-null    float64\n 9   diff_lag_1            508 non-null    float64\n 10  diff_lag_2            507 non-null    float64\n 11  diff_lag_3            506 non-null    float64\n 12  diff_lag_4            505 non-null    float64\n 13  diff_lag_5            504 non-null    float64\n 14  diff_lag_6            503 non-null    float64\n 15  diff_lag_7            502 non-null    float64\n 16  diff_lag_8            501 non-null    float64\n 17  diff_lag_9            500 non-null    float64\n 18  diff_lag_10           499 non-null    float64\n 19  diff_lag_11           498 non-null    float64\n 20  diff_lag_12           497 non-null    float64\n 21  diff_lag_13           496 non-null    float64\n 22  diff_lag_14           495 non-null    float64\n 23  diff_lag_15           494 non-null    float64\n 24  diff_lag_16           493 non-null    float64\n 25  diff_lag_17           492 non-null    float64\n 26  diff_lag_18           491 non-null    float64\n 27  diff_lag_19           490 non-null    float64\n 28  diff_lag_20           489 non-null    float64\n 29  diff_lag_21           488 non-null    float64\n 30  diff_lag_22           487 non-null    float64\n 31  diff_lag_23           486 non-null    float64\n 32  diff_lag_24           485 non-null    float64\n 33  diff_lag_25           484 non-null    float64\n 34  diff_lag_26           483 non-null    float64\n 35  diff_lag_27           482 non-null    float64\n 36  diff_lag_28           481 non-null    float64\n 37  diff_lag_29           480 non-null    float64\n 38  diff_lag_30           479 non-null    float64\n 39  diff_lag_31           478 non-null    float64\n 40  diff_lag_32           477 non-null    float64\n 41  diff_lag_33           476 non-null    float64\n 42  diff_lag_34           475 non-null    float64\n 43  diff_lag_35           474 non-null    float64\n 44  diff_lag_36           473 non-null    float64\n 45  diff_lag_37           472 non-null    float64\n 46  diff_lag_38           471 non-null    float64\n 47  diff_lag_39           470 non-null    float64\n 48  diff_lag_40           469 non-null    float64\n 49  diff_lag_41           468 non-null    float64\n 50  diff_lag_42           467 non-null    float64\n 51  diff_lag_43           466 non-null    float64\n 52  diff_lag_44           465 non-null    float64\n 53  diff_lag_45           464 non-null    float64\n 54  diff_lag_46           463 non-null    float64\n 55  diff_lag_47           462 non-null    float64\n 56  diff_lag_48           461 non-null    float64\n 57  diff_lag_49           460 non-null    float64\n 58  diff_lag_50           459 non-null    float64\n 59  diff_lag_51           458 non-null    float64\n 60  diff_lag_52           457 non-null    float64\n 61  diff_lag_53           456 non-null    float64\n 62  diff_lag_54           455 non-null    float64\n 63  diff_lag_55           454 non-null    float64\n 64  diff_lag_56           453 non-null    float64\n 65  diff_lag_57           452 non-null    float64\n 66  diff_lag_58           451 non-null    float64\n 67  diff_lag_59           450 non-null    float64\n 68  diff_lag_60           449 non-null    float64\n 69  max_high_6            496 non-null    float64\n 70  min_low_6             496 non-null    float64\n 71  max_high_20           479 non-null    float64\n 72  min_low_20            479 non-null    float64\n 73  upward_move_6         496 non-null    float64\n 74  downward_move_6       496 non-null    float64\n 75  upward_move_20        479 non-null    float64\n 76  downward_move_20      479 non-null    float64\n 77  bullish_move_20       479 non-null    float64\n 78  bearish_move_20       479 non-null    float64\n 79  bullish_move_flag_20  509 non-null    int64  \n 80  bearish_move_flag_20  509 non-null    int64  \n 81  volume_std_5          507 non-null    float64\n 82  volume_std_mean       509 non-null    float64\n 83  volume_mean_5         505 non-null    float64\n 84  volume_mean_3         507 non-null    float64\n 85  move_from_top         470 non-null    float64\n 86  move_from_bottom      470 non-null    float64\n 87  EMA_slope             509 non-null    float64\n 88  EMA_slope_15          509 non-null    float64\n 89  EMA_slope_60          509 non-null    float64\n 90  deviation_            509 non-null    int64  \ndtypes: float64(84), int64(6), object(1)\nmemory usage: 362.0+ KB\nNone\n\npredictions_df structure:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 509 entries, 0 to 508\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   datetime   509 non-null    object \n 1   bearish_0  509 non-null    float64\n 2   bearish_1  509 non-null    float64\n 3   bearish_2  509 non-null    float64\n 4   bearish_3  509 non-null    float64\n 5   bullish_0  509 non-null    float64\n 6   bullish_1  509 non-null    float64\n 7   bullish_2  509 non-null    float64\n 8   bullish_3  509 non-null    float64\n 9   y_true     509 non-null    int64  \ndtypes: float64(8), int64(1), object(1)\nmemory usage: 39.9+ KB\nNone\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "7e674d14",
        "execution_start": 1743977791370,
        "execution_millis": 3,
        "sql_integration_id": "",
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_variable_name": "",
        "cell_id": "2869860482ab472aa0fade144c05dbb8",
        "deepnote_cell_type": "code"
      },
      "source": "# Let's check if the datetime format is consistent and convert to datetime type\ndf['datetime'] = pd.to_datetime(df['datetime'])\npredictions_df['datetime'] = pd.to_datetime(predictions_df['datetime'])\n\n# Sort both dataframes by datetime to ensure proper alignment\ndf = df.sort_values('datetime').reset_index(drop=True)\npredictions_df = predictions_df.sort_values('datetime').reset_index(drop=True)\n\n# Check if the datetimes match between the two dataframes\nprint(\"First few datetimes from df:\")\nprint(df['datetime'].head())\nprint(\"\\nFirst few datetimes from predictions_df:\")\nprint(predictions_df['datetime'].head())\n\n# Check if the lengths match\nprint(\"\\nLength of df:\", len(df))\nprint(\"Length of predictions_df:\", len(predictions_df))\n\n# Check if datetimes are exactly equal\nprint(\"\\nAll datetimes match between dataframes:\", \n      (df['datetime'] == predictions_df['datetime']).all())",
      "block_group": "4f63fa0f7220481781403cdd27dc6703",
      "execution_count": 57,
      "outputs": [
        {
          "name": "stdout",
          "text": "First few datetimes from df:\n0   2025-04-03 03:20:00+00:00\n1   2025-04-03 03:25:00+00:00\n2   2025-04-03 03:30:00+00:00\n3   2025-04-03 03:35:00+00:00\n4   2025-04-03 03:40:00+00:00\nName: datetime, dtype: datetime64[ns, UTC]\n\nFirst few datetimes from predictions_df:\n0   2025-04-03 03:20:00+00:00\n1   2025-04-03 03:25:00+00:00\n2   2025-04-03 03:30:00+00:00\n3   2025-04-03 03:35:00+00:00\n4   2025-04-03 03:40:00+00:00\nName: datetime, dtype: datetime64[ns, UTC]\n\nLength of df: 509\nLength of predictions_df: 509\n\nAll datetimes match between dataframes: True\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "a6e19d97",
        "execution_start": 1743977811965,
        "execution_millis": 1159,
        "sql_integration_id": "",
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_variable_name": "",
        "cell_id": "e46d667989314d4ab5218fd02d1bd8b1",
        "deepnote_cell_type": "code"
      },
      "source": "# Let's modify the DataHandler's get_and_prepare_data method to handle the datetime properly\nclass DataHandler:\n    def __init__(self):\n        self.scaler = None\n        self.raw_data_path = 'data/raw/base_data.csv'\n        self.processed_data_path = 'data/processed/transformed_data.csv'\n    \n    def get_and_prepare_data(self, lookback, access_token):\n        from Moduled_functions import tranformation\n        try:\n            base_data, current_price = get_data(lookback, access_token)\n            base_data['datetime'] = pd.to_datetime(base_data['datetime'])\n            \n            try:\n                existing_data = pd.read_csv(self.raw_data_path)\n                existing_data['datetime'] = pd.to_datetime(existing_data['datetime'])\n                \n                # Add new records\n                new_records = base_data[~base_data['datetime'].isin(existing_data['datetime'])]\n                if not new_records.empty:\n                    updated_data = pd.concat([existing_data, new_records])\n                    updated_data = updated_data.sort_values('datetime').reset_index(drop=True)\n                else:\n                    updated_data = existing_data.sort_values('datetime').reset_index(drop=True)\n            except FileNotFoundError:\n                updated_data = base_data.sort_values('datetime').reset_index(drop=True)\n\n            updated_data.to_csv(self.raw_data_path, index=False)\n            logger.info(f\"Raw data saved to {self.raw_data_path}\")\n            \n            base_data = tranformation(base_data, 0.0015)            \n            return base_data, current_price\n        except Exception as e:\n            logger.error(f\"Error in get_and_prepare_data: {str(e)}\")\n            raise\n\n    def calculate_technical_indicators(self, data):\n        try:\n            data['datetime'] = pd.to_datetime(data['datetime'])\n            data['EMA_slope'] = calculate_ema_slope(data, 'Open', 9)\n            data['EMA_slope_15'] = calculate_ema_slope(data, 'Open', 15)\n            data['EMA_slope_60'] = calculate_ema_slope(data, 'Open', 60)\n            data['deviation_'] = data['bullish_move_flag_20'] + data['bearish_move_flag_20']\n            \n            try:\n                existing_data = pd.read_csv(self.processed_data_path)\n                existing_data['datetime'] = pd.to_datetime(existing_data['datetime'])\n                \n                new_records = data[~data['datetime'].isin(existing_data['datetime'])]\n                if not new_records.empty:\n                    updated_data = pd.concat([existing_data, new_records])\n                    updated_data = updated_data.sort_values('datetime').reset_index(drop=True)\n                else:\n                    updated_data = existing_data.sort_values('datetime').reset_index(drop=True)\n            except FileNotFoundError:\n                updated_data = data.sort_values('datetime').reset_index(drop=True)\n\n            updated_data.to_csv(self.processed_data_path, index=False)\n            logger.info(f\"Processed data saved to {self.processed_data_path}\")\n            return updated_data\n        except Exception as e:\n            logger.error(f\"Error in calculate_technical_indicators: {str(e)}\")\n            raise\n\n# Create new instance of TradingSystem with modified DataHandler\ntrading_system = TradingSystem(access_token, bearish_model, bullish_model)\n\n# Try executing the system again\nprint(\"Attempting to execute trading system with modified DataHandler...\")\ntrading_system.execute(lookback=100)",
      "block_group": "0f68b2afa1be4018ae5fc1936c04ce1b",
      "execution_count": 59,
      "outputs": [
        {
          "name": "stdout",
          "text": "Attempting to execute trading system with modified DataHandler...\n2025-04-06 22:16:52,596 - INFO - Raw data saved to data/raw/base_data.csv\n2025-04-06 22:16:53,117 - INFO - Processed data saved to data/processed/transformed_data.csv\n2025-04-06 22:16:53,119 - ERROR - Error in execute: 'DataHandler' object has no attribute 'normalize_features'\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "4381db4e",
        "execution_start": 1743977829235,
        "execution_millis": 1805,
        "sql_integration_id": "",
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_variable_name": "",
        "cell_id": "dea3632267aa4885a01f1dbd3ec09f92",
        "deepnote_cell_type": "code"
      },
      "source": "# Let's fix the DataHandler by adding the missing normalize_features method\nclass DataHandler:\n    def __init__(self):\n        self.scaler = None\n        self.raw_data_path = 'data/raw/base_data.csv'\n        self.processed_data_path = 'data/processed/transformed_data.csv'\n    \n    def get_and_prepare_data(self, lookback, access_token):\n        from Moduled_functions import tranformation\n        try:\n            base_data, current_price = get_data(lookback, access_token)\n            base_data['datetime'] = pd.to_datetime(base_data['datetime'])\n            \n            try:\n                existing_data = pd.read_csv(self.raw_data_path)\n                existing_data['datetime'] = pd.to_datetime(existing_data['datetime'])\n                \n                new_records = base_data[~base_data['datetime'].isin(existing_data['datetime'])]\n                if not new_records.empty:\n                    updated_data = pd.concat([existing_data, new_records])\n                    updated_data = updated_data.sort_values('datetime').reset_index(drop=True)\n                else:\n                    updated_data = existing_data.sort_values('datetime').reset_index(drop=True)\n            except FileNotFoundError:\n                updated_data = base_data.sort_values('datetime').reset_index(drop=True)\n\n            updated_data.to_csv(self.raw_data_path, index=False)\n            logger.info(f\"Raw data saved to {self.raw_data_path}\")\n            \n            base_data = tranformation(base_data, 0.0015)            \n            return base_data, current_price\n        except Exception as e:\n            logger.error(f\"Error in get_and_prepare_data: {str(e)}\")\n            raise\n\n    def calculate_technical_indicators(self, data):\n        try:\n            data['datetime'] = pd.to_datetime(data['datetime'])\n            data['EMA_slope'] = calculate_ema_slope(data, 'Open', 9)\n            data['EMA_slope_15'] = calculate_ema_slope(data, 'Open', 15)\n            data['EMA_slope_60'] = calculate_ema_slope(data, 'Open', 60)\n            data['deviation_'] = data['bullish_move_flag_20'] + data['bearish_move_flag_20']\n            \n            try:\n                existing_data = pd.read_csv(self.processed_data_path)\n                existing_data['datetime'] = pd.to_datetime(existing_data['datetime'])\n                \n                new_records = data[~data['datetime'].isin(existing_data['datetime'])]\n                if not new_records.empty:\n                    updated_data = pd.concat([existing_data, new_records])\n                    updated_data = updated_data.sort_values('datetime').reset_index(drop=True)\n                else:\n                    updated_data = existing_data.sort_values('datetime').reset_index(drop=True)\n            except FileNotFoundError:\n                updated_data = data.sort_values('datetime').reset_index(drop=True)\n\n            updated_data.to_csv(self.processed_data_path, index=False)\n            logger.info(f\"Processed data saved to {self.processed_data_path}\")\n            return updated_data\n        except Exception as e:\n            logger.error(f\"Error in calculate_technical_indicators: {str(e)}\")\n            raise\n            \n    def normalize_features(self, data):\n        try:\n            # Select features (columns 9-69)\n            feature_columns = data.columns[9:69]\n            X = data[feature_columns]\n            y = data['deviation_']\n            \n            if self.scaler is None:\n                self.scaler = StandardScaler()\n                self.scaler.fit(X)\n            \n            X_normalized = pd.DataFrame(\n                self.scaler.transform(X),\n                columns=feature_columns\n            ).round(1)\n            \n            return X_normalized, y\n        except Exception as e:\n            logger.error(f\"Error in normalize_features: {str(e)}\")\n            raise\n\n# Create new instance of TradingSystem with complete DataHandler\ntrading_system = TradingSystem(access_token, bearish_model, bullish_model)\n\n# Try executing the system again\nprint(\"Attempting to execute trading system with complete DataHandler...\")\ntrading_system.execute(lookback=100)",
      "block_group": "1b81eccabde4498b8401ee720b911378",
      "execution_count": 61,
      "outputs": [
        {
          "name": "stdout",
          "text": "Attempting to execute trading system with complete DataHandler...\n2025-04-06 22:17:09,703 - INFO - Raw data saved to data/raw/base_data.csv\n2025-04-06 22:17:10,213 - INFO - Processed data saved to data/processed/transformed_data.csv\n2025-04-06 22:17:10,827 - ERROR - Error processing strategy volume_based: unsupported operand type(s) for |: 'float' and 'float'\n2025-04-06 22:17:10,942 - ERROR - Error processing strategy volume_based_hedge: unsupported operand type(s) for |: 'float' and 'float'\n2025-04-06 22:17:11,033 - ERROR - Error processing strategy ema_slope: name 'bearish_threshold_min' is not defined\n2025-04-06 22:17:11,035 - INFO - Execution completed at 2025-04-06 22:17:11.035927\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "bb434d5f",
        "execution_start": 1743977858935,
        "execution_millis": 0,
        "sql_integration_id": "",
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_variable_name": "",
        "cell_id": "d373f28588e8461dbb0e75539aa55c21",
        "deepnote_cell_type": "code"
      },
      "source": "# Fix the strategy classes with proper operator usage and defined variables\nclass VolumeBasedStrategy(BaseStrategy):\n    def __init__(self):\n        super().__init__('volume_based')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': 450,\n            'ema_slope_15_threshold_max': 750,\n            'ema_slope_15_threshold_min': -750,\n            'ema_slope_60_threshold': 0.50,\n            'bearish_threshold_min': 0.5,\n            'bearish_threshold_max': 0.65,\n            'volume_threshold': 500,\n            'take_profit_multiplier': 1.0015,\n            'stop_loss_multiplier': 0.0005,\n            'quantity': '1000000',\n            'account_id': account_id  # Use the global account_id\n        }\n    \n    def generate_signals(self, data):\n        config = self.config\n        \n        # Generate bearish signals using logical or with numpy\n        bearish_signals = data.apply(lambda x: 2 if (\n            x['bearish_2'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        # Generate bullish signals using logical or with numpy\n        bullish_signals = data.apply(lambda x: 1 if (\n            x['bullish_1'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n        \n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n\nclass VolumeBasedStrategyHedge(BaseStrategy):\n    def __init__(self):\n        super().__init__('volume_based_hedge')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': 450,\n            'ema_slope_15_threshold_max': 750,\n            'ema_slope_15_threshold_min': -750,\n            'ema_slope_60_threshold': 0.50,\n            'bearish_threshold_min': 0.5,\n            'bearish_threshold_max': 0.65,\n            'volume_threshold': 500,\n            'take_profit_multiplier': 1.0015,\n            'stop_loss_multiplier': 0.0005,\n            'quantity': '1000000',\n            'account_id': account_id  # Use the global account_id\n        }\n    \n    def generate_signals(self, data):\n        config = self.config\n        \n        # Generate bearish signals using logical or with numpy\n        bearish_signals = data.apply(lambda x: 2 if (\n            x['bearish_2'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        # Generate bullish signals using logical or with numpy\n        bullish_signals = data.apply(lambda x: 1 if (\n            x['bullish_1'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n\nclass EMASlopeStrategy(BaseStrategy):\n    def __init__(self):\n        super().__init__('ema_slope')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': -0.0002,\n            'ema_slope_15_threshold': -0.0001,\n            'ema_slope_60_threshold': 0,\n            'volume_mean_5_min': 450,\n            'volume_mean_5_max': 700,\n            'bearish_threshold_min': 0.5,\n            'bearish_threshold_max': 0.65,\n            'move_from_top_min': 0.08,\n            'move_from_top_max': 0.25,\n            'move_from_bottom_min': 0.08,\n            'move_from_bottom_max': 0.25,\n            'volume_min': 450,\n            'volume_max': 700,\n            'volume_mean_3_min': 500,\n            'volume_mean_3_max': 750,\n            'take_profit_multiplier': 1.002,\n            'stop_loss_multiplier': 0.001,\n            'quantity': '500000',\n            'account_id': account_id  # Use the global account_id\n        }\n    \n    def generate_signals(self, data):\n        config = self.config\n        \n        # Generate bearish signals\n        bearish_signals = data.apply(lambda x: 2 if (\n            x['volume_mean_5'] > config['volume_mean_5_min'] and\n            x['volume_mean_5'] < config['volume_mean_5_max'] and\n            x['bearish_2'] > config['bearish_threshold_min'] and\n            x['bearish_2'] < config['bearish_threshold_max'] and\n            x['move_from_top'] > config['move_from_top_min'] and\n            x['move_from_top'] < config['move_from_top_max'] and\n            x['volume'] > config['volume_min'] and\n            x['volume'] < config['volume_max'] and\n            x['volume_mean_3'] > config['volume_mean_3_min'] and\n            x['volume_mean_3'] < config['volume_mean_3_max']\n        ) else 0, axis=1)\n\n        # Generate bullish signals\n        bullish_signals = data.apply(lambda x: 1 if (\n            x['volume'] >= config['volume_min'] and\n            x['volume'] <= config['volume_max'] and\n            x['volume_mean_3'] >= config['volume_mean_3_min'] and\n            x['volume_mean_3'] <= config['volume_mean_3_max'] and\n            x['volume_mean_5'] >= config['volume_mean_5_min'] and\n            x['volume_mean_5'] <= config['volume_mean_5_max'] and\n            x['bullish_1'] >= config['bearish_threshold_min'] and\n            x['bullish_1'] <= config['bearish_threshold_max'] and\n            x['move_from_bottom'] >= config['move_from_bottom_min'] and\n            x['move_from_bottom'] <= config['move_from_bottom_max']\n        ) else 0, axis=1)\n\n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n\n# Re-register the fixed strategies\nvolume_strategy = VolumeBasedStrategy()\nvolume_strategy_hedge = VolumeBasedStrategyHedge()\nema_strategy = EMASlopeStrategy()\n\nStrategyRegistry.register(volume_strategy)\nStrategyRegistry.register(volume_strategy_hedge)\nStrategyRegistry.register(ema_strategy)\n\n# Try executing the system again\nprint(\"Attempting to execute trading system with fixed strategies...\")\ntrading_system.execute(lookback=100)",
      "block_group": "a2652491bee445cc9297dc10bd6c7a79",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Can't instantiate abstract class VolumeBasedStrategy with abstract method get_order_parameters",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[63], line 161\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m signals\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Re-register the fixed strategies\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m volume_strategy \u001b[38;5;241m=\u001b[39m \u001b[43mVolumeBasedStrategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m volume_strategy_hedge \u001b[38;5;241m=\u001b[39m VolumeBasedStrategyHedge()\n\u001b[1;32m    163\u001b[0m ema_strategy \u001b[38;5;241m=\u001b[39m EMASlopeStrategy()\n",
            "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class VolumeBasedStrategy with abstract method get_order_parameters"
          ]
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "a40b092d",
        "execution_start": 1743977892195,
        "execution_millis": 2170,
        "sql_integration_id": "",
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_variable_name": "",
        "cell_id": "d7ca14940b9d41e8a5e2310ea03a83c3",
        "deepnote_cell_type": "code"
      },
      "source": "# Add the missing get_order_parameters method to all strategy classes\nclass VolumeBasedStrategy(BaseStrategy):\n    def __init__(self):\n        super().__init__('volume_based')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': 450,\n            'ema_slope_15_threshold_max': 750,\n            'ema_slope_15_threshold_min': -750,\n            'ema_slope_60_threshold': 0.50,\n            'bearish_threshold_min': 0.5,\n            'bearish_threshold_max': 0.65,\n            'volume_threshold': 500,\n            'take_profit_multiplier': 1.0015,\n            'stop_loss_multiplier': 0.0005,\n            'quantity': '1000000',\n            'account_id': account_id\n        }\n    \n    def generate_signals(self, data):\n        config = self.config\n        \n        bearish_signals = data.apply(lambda x: 2 if (\n            x['bearish_2'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        bullish_signals = data.apply(lambda x: 1 if (\n            x['bullish_1'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n        \n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n    \n    def get_order_parameters(self, current_price, signal):\n        config = self.config\n        \n        if signal == 1:  # Bullish\n            return {\n                'take_profit': current_price * config['take_profit_multiplier'],\n                'stop_loss': current_price * (1 - config['stop_loss_multiplier']),\n                'quantity': config['quantity'],\n                'type': 'BUY',\n                'account_id': config['account_id']\n            }\n        elif signal == 2:  # Bearish\n            return {\n                'take_profit': current_price * (1 - config['stop_loss_multiplier']),\n                'stop_loss': current_price * config['take_profit_multiplier'],\n                'quantity': f\"-{config['quantity']}\",\n                'type': 'SELL',\n                'account_id': config['account_id']\n            }\n        return None\n\nclass VolumeBasedStrategyHedge(BaseStrategy):\n    def __init__(self):\n        super().__init__('volume_based_hedge')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': 450,\n            'ema_slope_15_threshold_max': 750,\n            'ema_slope_15_threshold_min': -750,\n            'ema_slope_60_threshold': 0.50,\n            'bearish_threshold_min': 0.5,\n            'bearish_threshold_max': 0.65,\n            'volume_threshold': 500,\n            'take_profit_multiplier': 1.0015,\n            'stop_loss_multiplier': 0.0005,\n            'quantity': '1000000',\n            'account_id': account_id\n        }\n    \n    def generate_signals(self, data):\n        config = self.config\n        \n        bearish_signals = data.apply(lambda x: 2 if (\n            x['bearish_2'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        bullish_signals = data.apply(lambda x: 1 if (\n            x['bullish_1'] > config['bearish_threshold_min'] and\n            (x['EMA_slope_15'] > config['ema_slope_15_threshold_max'] or\n             x['EMA_slope_15'] < config['ema_slope_15_threshold_min']) and\n            x['volume'] > config['volume_threshold']\n        ) else 0, axis=1)\n\n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n    \n    def get_order_parameters(self, current_price, signal):\n        config = self.config\n        \n        if signal == 2:  # Bearish\n            return {\n                'take_profit': current_price * config['take_profit_multiplier'],\n                'stop_loss': current_price * (1 - config['stop_loss_multiplier']),\n                'quantity': config['quantity'],\n                'type': 'BUY',\n                'account_id': config['account_id']\n            }\n        elif signal == 1:  # Bullish\n            return {\n                'take_profit': current_price * (1 - config['stop_loss_multiplier']),\n                'stop_loss': current_price * config['take_profit_multiplier'],\n                'quantity': f\"-{config['quantity']}\",\n                'type': 'SELL',\n                'account_id': config['account_id']\n            }\n        return None\n\nclass EMASlopeStrategy(BaseStrategy):\n    def __init__(self):\n        super().__init__('ema_slope')\n    \n    def get_default_config(self):\n        return {\n            'ema_slope_threshold': -0.0002,\n            'ema_slope_15_threshold': -0.0001,\n            'ema_slope_60_threshold': 0,\n            'volume_mean_5_min': 450,\n            'volume_mean_5_max': 700,\n            'bearish_threshold_min': 0.5,\n            'bearish_threshold_max': 0.65,\n            'move_from_top_min': 0.08,\n            'move_from_top_max': 0.25,\n            'move_from_bottom_min': 0.08,\n            'move_from_bottom_max': 0.25,\n            'volume_min': 450,\n            'volume_max': 700,\n            'volume_mean_3_min': 500,\n            'volume_mean_3_max': 750,\n            'take_profit_multiplier': 1.002,\n            'stop_loss_multiplier': 0.001,\n            'quantity': '500000',\n            'account_id': account_id\n        }\n    \n    def generate_signals(self, data):\n        config = self.config\n        \n        bearish_signals = data.apply(lambda x: 2 if (\n            x['volume_mean_5'] > config['volume_mean_5_min'] and\n            x['volume_mean_5'] < config['volume_mean_5_max'] and\n            x['bearish_2'] > config['bearish_threshold_min'] and\n            x['bearish_2'] < config['bearish_threshold_max'] and\n            x['move_from_top'] > config['move_from_top_min'] and\n            x['move_from_top'] < config['move_from_top_max'] and\n            x['volume'] > config['volume_min'] and\n            x['volume'] < config['volume_max'] and\n            x['volume_mean_3'] > config['volume_mean_3_min'] and\n            x['volume_mean_3'] < config['volume_mean_3_max']\n        ) else 0, axis=1)\n\n        bullish_signals = data.apply(lambda x: 1 if (\n            x['volume'] >= config['volume_min'] and\n            x['volume'] <= config['volume_max'] and\n            x['volume_mean_3'] >= config['volume_mean_3_min'] and\n            x['volume_mean_3'] <= config['volume_mean_3_max'] and\n            x['volume_mean_5'] >= config['volume_mean_5_min'] and\n            x['volume_mean_5'] <= config['volume_mean_5_max'] and\n            x['bullish_1'] >= config['bearish_threshold_min'] and\n            x['bullish_1'] <= config['bearish_threshold_max'] and\n            x['move_from_bottom'] >= config['move_from_bottom_min'] and\n            x['move_from_bottom'] <= config['move_from_bottom_max']\n        ) else 0, axis=1)\n\n        signals = pd.DataFrame({\n            'datetime': data['datetime'],\n            'bearish_flag': bearish_signals,\n            'bullish_flag': bullish_signals\n        })\n        \n        return signals\n    \n    def get_order_parameters(self, current_price, signal):\n        config = self.config\n        \n        if signal == 1:  # Bullish\n            return {\n                'take_profit': current_price * config['take_profit_multiplier'],\n                'stop_loss': current_price * (1 - config['stop_loss_multiplier']),\n                'quantity': config['quantity'],\n                'type': 'BUY',\n                'account_id': config['account_id']\n            }\n        elif signal == 2:  # Bearish\n            return {\n                'take_profit': current_price * (1 - config['stop_loss_multiplier']),\n                'stop_loss': current_price * config['take_profit_multiplier'],\n                'quantity': f\"-{config['quantity']}\",\n                'type': 'SELL',\n                'account_id': config['account_id']\n            }\n        return None\n\n# Re-register the complete strategies\nvolume_strategy = VolumeBasedStrategy()\nvolume_strategy_hedge = VolumeBasedStrategyHedge()\nema_strategy = EMASlopeStrategy()\n\nStrategyRegistry.register(volume_strategy)\nStrategyRegistry.register(volume_strategy_hedge)\nStrategyRegistry.register(ema_strategy)\n\n# Try executing the system again\nprint(\"Attempting to execute trading system with complete strategies...\")\ntrading_system.execute(lookback=100)",
      "block_group": "5887714669754919bb1552dec3d66df0",
      "execution_count": 65,
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-06 22:18:12,708 - INFO - Strategy registered: volume_based\n2025-04-06 22:18:12,709 - INFO - Strategy registered: volume_based_hedge\n2025-04-06 22:18:12,710 - INFO - Strategy registered: ema_slope\nAttempting to execute trading system with complete strategies...\n2025-04-06 22:18:13,157 - INFO - Raw data saved to data/raw/base_data.csv\n2025-04-06 22:18:13,772 - INFO - Processed data saved to data/processed/transformed_data.csv\n2025-04-06 22:18:14,298 - ERROR - Error processing strategy volume_based: 'bearish_2'\n2025-04-06 22:18:14,302 - ERROR - Error processing strategy volume_based_hedge: 'bearish_2'\n2025-04-06 22:18:14,305 - ERROR - Error processing strategy ema_slope: 'bearish_2'\n2025-04-06 22:18:14,307 - INFO - Execution completed at 2025-04-06 22:18:14.307344\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "8e433f4a",
        "execution_start": 1743977900150,
        "execution_millis": 214,
        "sql_integration_id": "",
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_variable_name": "",
        "cell_id": "e2ed5485b0a745eb9a6fe5bbbb6d3afc",
        "deepnote_cell_type": "code"
      },
      "source": "# Let's check the data structure and available columns\ndata = pd.read_csv('data/predictions/predictions_data.csv')\nprint(\"Columns in predictions data:\")\nprint(data.columns.tolist())\n\n# Check the first few rows of predictions data\nprint(\"\\nFirst few rows of predictions data:\")\nprint(data.head())",
      "block_group": "c1f388f430814acb9215baced92a2ec2",
      "execution_count": 67,
      "outputs": [
        {
          "name": "stdout",
          "text": "Columns in predictions data:\n['datetime', 'bearish_0', 'bearish_1', 'bearish_2', 'bearish_3', 'bullish_0', 'bullish_1', 'bullish_2', 'bullish_3', 'y_true']\n\nFirst few rows of predictions data:\n                    datetime  bearish_0     bearish_1  bearish_2  \\\n0  2025-04-03 03:20:00+00:00   0.605764  9.990175e-11   0.394236   \n1  2025-04-03 03:25:00+00:00   0.605764  9.990175e-11   0.394236   \n2  2025-04-03 03:30:00+00:00   0.605764  9.990175e-11   0.394236   \n3  2025-04-03 03:35:00+00:00   0.605764  9.990175e-11   0.394236   \n4  2025-04-03 03:40:00+00:00   0.605764  9.990175e-11   0.394236   \n\n      bearish_3  bullish_0  bullish_1     bullish_2     bullish_3  y_true  \n0  9.990175e-11   0.658143   0.341857  1.216900e-10  1.217527e-10       0  \n1  9.990175e-11   0.658143   0.341857  1.216900e-10  1.217527e-10       0  \n2  9.990175e-11   0.658143   0.341857  1.216900e-10  1.217527e-10       0  \n3  9.990175e-11   0.658143   0.341857  1.216900e-10  1.217527e-10       0  \n4  9.990175e-11   0.658143   0.341857  1.216900e-10  1.217527e-10       1  \n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "2db042da",
        "execution_start": 1743977927625,
        "execution_millis": 3142,
        "sql_integration_id": "",
        "execution_context_id": "877e3c38-7686-489a-933d-a9bb8bf843fc",
        "deepnote_variable_name": "",
        "cell_id": "8f31be12726d43e5be36008aff17fb75",
        "deepnote_cell_type": "code"
      },
      "source": "# Let's modify the TradingSystem class to properly merge the prediction data with the base data\nclass TradingSystem:\n    def __init__(self, access_token, bearish_model, bullish_model):\n        self.data_handler = DataHandler()\n        self.order_manager = OrderManager(access_token)\n        self.predictions_data_path = 'data/predictions/predictions_data.csv'\n        \n    def execute(self, lookback=100):\n        try:\n            # Get and prepare data\n            base_data, current_price = self.data_handler.get_and_prepare_data(lookback, self.order_manager.access_token)\n            base_data = self.data_handler.calculate_technical_indicators(base_data)\n            X_normalized, y = self.data_handler.normalize_features(base_data)\n            \n            # Generate predictions\n            y_pred_bearish = bearish_model.predict(X_normalized, verbose=0)\n            y_pred_bearish = pd.DataFrame(y_pred_bearish)\n            y_pred_bullish = bullish_model.predict(X_normalized, verbose=0)\n            y_pred_bullish = pd.DataFrame(y_pred_bullish)\n            \n            # Combine results with datetime from base_data\n            result_combined = pd.concat([\n                base_data['datetime'].reset_index(drop=True),\n                pd.DataFrame(y_pred_bearish, columns=['bearish_0', 'bearish_1', 'bearish_2', 'bearish_3']),\n                pd.DataFrame(y_pred_bullish, columns=['bullish_0', 'bullish_1', 'bullish_2', 'bullish_3']),\n                pd.Series(y, name='y_true')\n            ], axis=1)\n            \n            try:\n                # Read existing predictions history\n                existing_preds = pd.read_csv(self.predictions_data_path)\n                existing_preds['datetime'] = pd.to_datetime(existing_preds['datetime'])\n                result_combined['datetime'] = pd.to_datetime(result_combined['datetime'])\n                \n                # Add new predictions\n                new_preds = result_combined[~result_combined['datetime'].isin(existing_preds['datetime'])]\n                if not new_preds.empty:\n                    updated_preds = pd.concat([existing_preds, new_preds])\n                    updated_preds = updated_preds.sort_values('datetime').reset_index(drop=True)\n                else:\n                    updated_preds = existing_preds\n            except FileNotFoundError:\n                updated_preds = result_combined\n            \n            updated_preds.to_csv(self.predictions_data_path, index=False)\n            \n            # Merge predictions with base data\n            base_data['datetime'] = pd.to_datetime(base_data['datetime'])\n            data_with_predictions = pd.merge(base_data, updated_preds, on='datetime', how='inner')\n            \n            # Process each strategy\n            for strategy in StrategyRegistry.get_all_strategies().values():\n                self._process_strategy(strategy, data_with_predictions, current_price)\n            \n            logger.info(f\"Execution completed at {datetime.now()}\")\n            \n        except Exception as e:\n            logger.error(f\"Error in execute: {str(e)}\")\n            raise\n    \n    def _process_strategy(self, strategy, data, current_price):\n        try:\n            # Generate signals\n            signals = strategy.generate_signals(data)\n            signals['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            strategy.save_signals(signals)\n            \n            # Check signals and execute trades\n            if not signals.empty:\n                latest_bearish = signals['bearish_flag'].iloc[-1]\n                latest_bullish = signals['bullish_flag'].iloc[-1]\n                \n                if latest_bullish == 1 and latest_bearish != 2:\n                    self._execute_trade(strategy, 1, current_price)\n                elif latest_bearish == 2 and latest_bullish != 1:\n                    self._execute_trade(strategy, 2, current_price)\n                    \n        except Exception as e:\n            logger.error(f\"Error processing strategy {strategy.name}: {str(e)}\")\n    \n    def _execute_trade(self, strategy, signal, current_price):\n        try:\n            order_params = strategy.get_order_parameters(current_price, signal)\n            if not order_params:\n                return\n            \n            order = self.order_manager.place_order(\n                current_price,\n                order_params['stop_loss'],\n                order_params['take_profit'],\n                order_params['quantity'],\n                order_params['account_id']\n            )\n            \n            order_id = order['orderCreateTransaction'].get('id')\n            order_time = order['orderCreateTransaction'].get('time')\n            account_id = order['orderCreateTransaction'].get('accountID')\n            \n            # Save trade details\n            trade_data = pd.DataFrame({\n                'Time': [order_time],\n                'Strategy': [strategy.name],\n                'Type': [order_params['type']],\n                'Quantity': [order_params['quantity']],\n                'Price': [current_price],\n                'Take_Profit': [order_params['take_profit']],\n                'Stop_Loss': [order_params['stop_loss']],\n                'Order_ID': [order_id],\n                'Account_ID': [account_id],\n            })\n            \n            strategy.save_trade(trade_data)\n            self.order_manager.save_order(trade_data)\n            \n            # Send notification\n            message = f\"{strategy.name} {order_params['type']} Order placed with order no: {order_id}\"\n            requests.get(f'https://api.day.app/iFbt9PqBdm6d2YvUT4irnN/{order_params[\"type\"]} Order Placed/{message}')\n            logger.info(message)\n            \n        except Exception as e:\n            logger.error(f\"Error executing trade for strategy {strategy.name}: {str(e)}\")\n\n# Create new instance of TradingSystem and execute\ntrading_system = TradingSystem(access_token, bearish_model, bullish_model)\nprint(\"Attempting to execute trading system with updated data handling...\")\ntrading_system.execute(lookback=100)",
      "block_group": "59a58f259f864bdd8d1c34c2114622f1",
      "execution_count": 69,
      "outputs": [
        {
          "name": "stdout",
          "text": "Attempting to execute trading system with updated data handling...\n2025-04-06 22:18:48,073 - INFO - Raw data saved to data/raw/base_data.csv\n2025-04-06 22:18:48,644 - INFO - Processed data saved to data/processed/transformed_data.csv\n2025-04-06 22:18:49,754 - INFO - Signals saved for strategy volume_based\n2025-04-06 22:18:50,213 - INFO - Signals saved for strategy volume_based_hedge\n2025-04-06 22:18:50,761 - INFO - Signals saved for strategy ema_slope\n2025-04-06 22:18:50,762 - INFO - Execution completed at 2025-04-06 22:18:50.762763\n",
          "output_type": "stream"
        }
      ],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0bacd4575af64fe885356340250ba3a6",
        "deepnote_cell_type": "text-cell-p"
      },
      "source": "The trading system is now working correctly. The main issues were:\n\n1. Data synchronization issues between base data and predictions\n2. Missing method implementations in strategy classes\n3. Incorrect boolean operations in signal generation\n4. Proper datetime handling and merging of dataframes\n\nThe system now:\n1. Properly loads and processes the base data\n2. Generates and saves predictions\n3. Merges predictions with base data\n4. Successfully generates signals for all strategies\n5. Saves the signals without errors\n\nThe execution completed without any errors, and the signals were successfully saved for all three strategies (volume_based, volume_based_hedge, and ema_slope).",
      "block_group": "22d4c246ee7641bba5cffc6cf01b2c02"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a127aa9e-0a77-4af9-a6ce-85e7a9b74042' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "11990d6ad3584e009dbd2e0e8ac5dabf"
  }
}